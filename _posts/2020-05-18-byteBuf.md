---
1layout:     post
title:      netty源码分析
subtitle:   bytebuf和netty内存
date:       2020-05-18
author:     tryingpfq
header-img: img/post-bg-bytebuf.jpg
catalog: true
tags:
    - netty
---

### ByteBuf

#### API 和 类结构

* 先看下bytebuf里面的一些主要的api：

```java
<pre>
 *     文档里面有一些描述，可见bytebuf会有一些游标，比如readerIndex:表示可读的开始
     	writerIndex:表示可写的开始，capacity:表示容量。
 *      +-------------------+------------------+------------------+
 *      | discardable bytes |  readable bytes  |  writable bytes  |
 *      |                   |    (CONTENT)可读  |      可写        |
 *      +-------------------+------------------+------------------+
 *      |                   |                  |                  |
 *      0      <=      readerIndex   <=   writerIndex    <=    capacity
 * </pre>
public abstract class ByteBuf implements ReferenceCounted, Comparable<ByteBuf> {

    /**
     * 获取容量
     */
    public abstract int capacity();
     /**
     * 最大容量 因为bytebuf有个最大容量值
     */
    public abstract int maxCapacity();
    /**
     * 分配
     */
    public abstract ByteBufAllocator alloc();
     /**
     * 获取可读开始索引
     */
    public abstract int readerIndex();
     /**
     * 读取bytebuf 并且设置当前可是读位置
     */
    public abstract ByteBuf readerIndex(int readerIndex);
	 /**
     * 获取写索引
     */
    public abstract int writerIndex();
    /**
     * 当前可读的长度：wirteIndex - readIndex
     */
    public abstract int readableBytes();
    /**
     * 当前可写的长度：capacity-writeIndex
     */
    public abstract int writableBytes();
     * 这个就是会记录你当前的readerIndex
     */
    public abstract ByteBuf markReaderIndex();
    /**
     * 这个一般是在上面那个方法记录后，在读取数据完后，对readerIndex进行复原，当然还有write也可以，就
     * 不在累赘了
     */
    public abstract ByteBuf resetReaderIndex();
    
    // 获取指定索引的数据 这个方法readerIndex位置是不会做修改的
    public abstract byte  getByte(int index);
     /**
     * 读取一个byte,读索引会加1 ，readShort 索引就+2，int+4，就不累赘了，后面还有很多read方法的。
     * 具体看下api和实现就好了
     */
    public abstract byte  readByte();
    /**
     * 写一个byte writeIndex+1，writeShort writeIndex+2,同样很多write方法也不累赘了
     */
    public abstract ByteBuf writeByte(int value);
}
```

上面的read write set这些api,主要要注意的是index的改变情况。


* 再看一个上面api实现的一个脚手架吧，就是这抽象类，`io.netty.buffer.AbstractByteBuf`，这个ByteBuf大部分api都有实现，当然有些交给子类去实现。

  

* 看下类的继承关系

 ![bytebuf1](https://github.com/tryingpfq/tryingpfq.github.io/blob/master/picture/bytebuf1.jpg?raw=true)

这个图要先有个印象，最好能够记住，后面有些地方，会根据这个继承关系读来做说明的。



### netty 内存类别

这个问题想必面试的时候，问的也比较多，笼统的回答就是，堆内存和堆外内存。但我们可以在细分一下，其实看上面那个类图应该大概就能看的出，ByteBuf是一个抽下类，提供了一些基础性的api,而AbstractByteBuf则就是ByteBuf的一个基础实现的脚手架，接着下面这层，就是更加类别的一个实现类。所以我们可以从三个维度进行分类。

#### pooled和upooled

pooled是池化的，也就是对应的实现类，在分配byteBuf内存的时候，会从**预先分配好**的内存取一段连续内存，而upooled是非池化的，每次进行内存分配的时候，直接调用系统API，向**操作系统**申请一块内存。

从这维度，就只有两种，一个是pooled，另一个就是unpooled。

 #### unsafe 和 非unsafe

在上面的实现中，没有标明的就是非unsafe。在jdk中，有个unsafe对象，直接可以拿到对象的一个内存地址，再基于内存地址直接进行操作，相比速度会比较快一些。在这里，unsafe也就是直接操作分配的内存的地址，非unsafe是基于内存数据数组角标的一个访问(基于数组角标的访问，最后也是要算出内存地址的)。

可以简单看下getByte(),这两种类型不同的实现。

> unsafe
>
> buf的内存地址+偏移量 直接范围
>
> 非unsafe
>
> buf数组 + 下表
>
> 这里的buf，可能是ByteBuf 或者jdk的bytebuffer

`io.netty.buffer.PooledUnsafeHeapByteBuf#_getByte`

```java
  @Override
    protected byte _getByte(int index) {
        return UnsafeByteBufUtil.getByte(memory, idx(index));
    }

	 static byte getByte(byte[] array, int index) {
        return PlatformDependent.getByte(array, index);
    }

	public static byte getByte(byte[] data, int index) {
        return PlatformDependent0.getByte(data, index);
    }

	static byte getByte(byte[] data, int index) {
        //你看这里就是直接拿到内存地址了,并调用unsafe对象获取
        return UNSAFE.getByte(data, BYTE_ARRAY_BASE_OFFSET + index);
    }

```

`io.netty.buffer.PooledHeapByteBuf#_getByte`

```java
 @Override
    protected byte _getByte(int index) {
        return HeapByteBufUtil.getByte(memory, idx(index));
    }

  static byte getByte(byte[] memory, int index) {
      	// 你看这里就是直接数组下标
        return memory[index];
    }
```



#### Heap 和 Direct

这两个应该是平时用的最多的一个，看名字也应该知道意思，heap就是在jvm上进行分配的，Direct则是调用jdk的api进行分配的，这个内存是不被jvm控制的，所以不参与gc的，需要手动释放的，所以要注意这个问题，以防出现内存问题。继续看下这两个的实现代码。

`io.netty.buffer.UnpooledHeapByteBuf`

```java
byte[] array;// 这里是基于byte数组的
```

`io.netty.buffer.UnpooledDirectByteBuf`

```java
// 这个是jdk的buffer,堆外内存
ByteBuffer buffer; // accessed by UnpooledUnsafeNoCleanerDirectByteBuf.reallocateDirect()

// 可以看下这个分配过程
io.netty.buffer.Unpooled#directBuffer(int)
  
 public static ByteBuf directBuffer(int initialCapacity) {
        return ALLOC.directBuffer(initialCapacity);
    }

@Override
    public ByteBuf directBuffer(int initialCapacity, int maxCapacity) {
        if (initialCapacity == 0 && maxCapacity == 0) {
            return emptyBuf;
        }
        validate(initialCapacity, maxCapacity);
        return newDirectBuffer(initialCapacity, maxCapacity);
    }

 @Override
    protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {
        final ByteBuf buf;
        if (PlatformDependent.hasUnsafe()) {
            buf = noCleaner ? new InstrumentedUnpooledUnsafeNoCleanerDirectByteBuf(this, initialCapacity, maxCapacity) :
                    new InstrumentedUnpooledUnsafeDirectByteBuf(this, initialCapacity, maxCapacity);
        } else {
            buf = new InstrumentedUnpooledDirectByteBuf(this, initialCapacity, maxCapacity);
        }
        return disableLeakDetector ? buf : toLeakAwareBuffer(buf);
    }

 protected ByteBuffer allocateDirect(int initialCapacity) {
     // 这里面就是jdk分配了
        return ByteBuffer.allocateDirect(initialCapacity);
    }
```

分类就讲到这里了，后面还会对这些类型进行分析。



### 内存分配器

同样，可以先看下分配器的主要API

```java
public interface ByteBufAllocator {

    ByteBufAllocator DEFAULT = ByteBufUtil.DEFAULT_ALLOCATOR;

    /**
     * Allocate a {@link ByteBuf}. If it is a direct or heap buffer
     * depends on the actual implementation.
     * 看下英文注意 就是是堆内还是堆外内存，依靠的是具体的实现
     * 重载的方法就不看了，可能就是指定大小了
     */
    ByteBuf buffer();
    
    /**
     * Allocate a {@link ByteBuf}, preferably a direct buffer which is suitable for I/O.
     */
    ByteBuf ioBuffer();
    
     /**
     * Allocate a heap {@link ByteBuf}.
     */
    ByteBuf heapBuffer();
     /**
     * Allocate a direct {@link ByteBuf}.
     */
    ByteBuf directBuffer();
     /**
     * Allocate a {@link CompositeByteBuf}.
     * If it is a direct or heap buffer depends on the actual implementation.
     * 日常用的并不多
     */
    CompositeByteBuf compositeBuffer();

}
```

看完上面，相比基本的有了解，对于直接内存还是堆内内存，是依靠具体的实现来完成。看到上面，好像只看到区分一个维度那就是堆内和直接内存的区分，并没有看到pooled和unpooled,以及unsafe和非unsafe，netty是怎么区分的呢？看后面的分析，会有提到。

可以下看下这个抽象的实现。`io.netty.buffer.AbstractByteBufAllocator`，大部分功能都有实现。

```java
 	//这里就是依靠具体的实现 来区分是heap还是direct
	@Override
    public ByteBuf buffer() {
        if (directByDefault) {
            return directBuffer();
        }
        return heapBuffer();
    }

//看下directBuffer(),
 @Override
    public ByteBuf directBuffer(int initialCapacity, int maxCapacity) {
        if (initialCapacity == 0 && maxCapacity == 0) {
            return emptyBuf;
        }
        validate(initialCapacity, maxCapacity);
        //会调用这个方法来分配，这个其实是一个抽象方法，也是依靠子类的具体实现
        //有pooled 和 unpooled
        return newDirectBuffer(initialCapacity, maxCapacity);
    }
	
	//看下这两个抽象方法，其实现有pooled 和 unpooled,
	//可见 pooled和unpooled这个维度的区分，也是要依靠子类具体实现类
	protected abstract ByteBuf newHeapBuffer(int initialCapacity, int maxCapacity);
    protected abstract ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity);

```

* UML图
![bytebuf2](https://github.com/tryingpfq/tryingpfq.github.io/blob/master/picture/bytebuf2.jpg?raw=true)

  这个类结构继承关系还是比较简单一下，其实主要就是两个具体的实现Unpooled和pooled。其实这两个主要就是实现上面说的那两个抽象方法。unpooled直接是调用系统api进行分配， pooled分配是直接从一个预先分配好的内存(相当于是一个池子)中进行分配。那看到这，是不是还有一个unsafe这个维度没有区分，这里是netty自动区分的，也就是说如果底层可以拿到unsafe这个对象，就会分配unsafe,否则就是非unsafe，这里我们可以简单看下代码。

  `io.netty.buffer.UnpooledByteBufAllocator`

  ```java
   @Override
      protected ByteBuf newHeapBuffer(int initialCapacity, int maxCapacity) {
          return PlatformDependent.hasUnsafe() ?
                  new InstrumentedUnpooledUnsafeHeapByteBuf(this, initialCapacity, maxCapacity) :
                  new InstrumentedUnpooledHeapByteBuf(this, initialCapacity, maxCapacity);
      }
  //这里要注意点一点是，这个分配的返回还是一个ByteBuf对象，但是里面持有的是jdk
  //一个 java.nio.DirectByteBuffer#DirectByteBuffer(int)
   @Override
      protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {
          final ByteBuf buf;
          if (PlatformDependent.hasUnsafe()) {
              buf = noCleaner ? new InstrumentedUnpooledUnsafeNoCleanerDirectByteBuf(this, initialCapacity, maxCapacity) :
                      new InstrumentedUnpooledUnsafeDirectByteBuf(this, initialCapacity, maxCapacity);
          } else {
              buf = new InstrumentedUnpooledDirectByteBuf(this, initialCapacity, maxCapacity);
          }
          return disableLeakDetector ? buf : toLeakAwareBuffer(buf);
      }
  
  // 其实很明显了，就是看操作系统底层有没有unsafe对象
  //PlatformDependent.hasUnsafe()
   public static boolean hasUnsafe() {
          return UNSAFE_UNAVAILABILITY_CAUSE == null;
      }
  ```

  后面就具体分析这两个实现类，pooled和unpooled

  

#### UnpooledByteBufAllocator

 `io.netty.buffer.UnpooledByteBufAllocator`

* Heap的分配

  `io.netty.buffer.UnpooledByteBufAllocator#newHeapBuffer`

  ```java
  //对于heap上的分配，分配器会调用这个方法。
  	@Override
      protected ByteBuf newHeapBuffer(int initialCapacity, int maxCapacity) {
          return PlatformDependent.hasUnsafe() ?
                  new InstrumentedUnpooledUnsafeHeapByteBuf(this, initialCapacity, maxCapacity) :
                  new InstrumentedUnpooledHeapByteBuf(this, initialCapacity, maxCapacity);
          //其实上面是通过调用子类的构造
      }
  
  // 只能跟着构造进去看了 InstrumentedUnpooledHeapByteBuf
  // 最后会进到这个父类中
    public UnpooledHeapByteBuf(ByteBufAllocator alloc, int initialCapacity, int maxCapacity) {
          super(maxCapacity);
  
          if (initialCapacity > maxCapacity) {
              throw new IllegalArgumentException(String.format(
                      "initialCapacity(%d) > maxCapacity(%d)", initialCapacity, maxCapacity));
          }
  
          this.alloc = checkNotNull(alloc, "alloc");
        	//这里面，其实就是new 一个数组，在堆中，并且初始化大小
          setArray(allocateArray(initialCapacity));
        	// 设置索引
          setIndex(0, 0);
      }
  // 对于unasfe 和 非unsafe,这里就不再累赘了，上面其实有说到，你进去看下_getByte()方法就行，一个是直接操作内存地址，一个是操作数组
  ```

* Directd分配

  `io.netty.buffer.UnpooledByteBufAllocator#newDirectBuffer`

  ```java
  	//对于Direct上的分配，分配器会调用这个方法。
  	@Override
      protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {
          final ByteBuf buf;
          if (PlatformDependent.hasUnsafe()) {
              buf = noCleaner ? new InstrumentedUnpooledUnsafeNoCleanerDirectByteBuf(this, initialCapacity, maxCapacity) :
                      new InstrumentedUnpooledUnsafeDirectByteBuf(this, initialCapacity, maxCapacity);
          } else {
              buf = new InstrumentedUnpooledDirectByteBuf(this, initialCapacity, maxCapacity);
          }
          return disableLeakDetector ? buf : toLeakAwareBuffer(buf);
      }
  
  // 同样，我们看下这个InstrumentedUnpooledDirectByteBuf
  // 会进入这个构造方法。
   public UnpooledDirectByteBuf(ByteBufAllocator alloc, int initialCapacity, int maxCapacity) {
          super(maxCapacity);
          ObjectUtil.checkNotNull(alloc, "alloc");
          checkPositiveOrZero(initialCapacity, "initialCapacity");
          checkPositiveOrZero(maxCapacity, "maxCapacity");
          if (initialCapacity > maxCapacity) {
              throw new IllegalArgumentException(String.format(
                      "initialCapacity(%d) > maxCapacity(%d)", initialCapacity, maxCapacity));
          }
  		//设置分配器
          this.alloc = alloc;
       	// 然后看下这个方法就好
       	// 把jdk创建的堆外内存进行保存
          setByteBuffer(allocateDirect(initialCapacity), false);
      }
  	 /**
       * Allocate a new direct {@link ByteBuffer} with the given initialCapacity.
       */
      protected ByteBuffer allocateDirect(int initialCapacity) {
          return ByteBuffer.allocateDirect(initialCapacity);
      }
  	
  	// 这里可以再分析一下，如果是unsafe 和 非unsafe,其实上面内容都是一直的，就是
  	//在setByteBuffer这个方法中，有一些特殊操作，比如要保持内存地址这些。
  		
  	// 非unsafe
  	//io.netty.buffer.UnpooledDirectByteBuf#setByteBuffer
  	void setByteBuffer(ByteBuffer buffer, boolean tryFree) {
          if (tryFree) {
              ByteBuffer oldBuffer = this.buffer;
              if (oldBuffer != null) {
                  if (doNotFree) {
                      doNotFree = false;
                  } else {
                      freeDirect(oldBuffer);
                  }
              }
          }
  
          this.buffer = buffer;	//这里直接保存的
          tmpNioBuf = null;
          capacity = buffer.remaining();
      }
  	
  	// unsafe
  	//io.netty.buffer.UnpooledUnsafeDirectByteBuf#setByteBuffe    
      @Override
      final void setByteBuffer(ByteBuffer buffer, boolean tryFree) {
          super.setByteBuffer(buffer, tryFree);
          //计算内存地址 里面是通过unsafe 获取这个buffer的内存地址，并保存，
          // 在get的时候，可以直接拿到这个 内存地址+偏移量 就可以直接访问，速度更快。
          memoryAddress = PlatformDependent.directBufferAddress(buffer);
      }
  ```



#### PooledByteBufAllocator

上面分析了unpooled的内存分配器，其实相对来说还是比较简单的，这里就要开始更为复杂的Pooled。

同样，先看下那两个抽象的API实现。`io.netty.buffer.PooledByteBufAllocator`

再分析之前，有必要先来看看netty自己实现的一个线程局部缓存。因为后面的两个方法中，是没有加锁的，并且这里会有多线程调用问题，所以就要解决并发问题。毕竟是操作同一个池子，如果每个调用线程，有一个自己的cache，那么这个问题就解决了。下面先说下，分配的流程。



**1：线程局部缓存，PoolThreadLocalCache**

```java
final class PoolThreadLocalCache extends FastThreadLocal<PoolThreadCache> {
    private final boolean useCacheForAllThreads;
    PoolThreadLocalCache(boolean useCacheForAllThreads) {
        this.useCacheForAllThreads = useCacheForAllThreads;
    }
	
    //其实主要是这个方法了。 
    @Override
    protected synchronized PoolThreadCache initialValue() {
        //这里的heapArenas 和 directArenas 是传进来的，那么创建的地方是哪里呢
        //当然是在这个构造中 io.netty.buffer.PooledByteBufAllocator#PooledByteBufAllocator
        // 这两个其实是一个数组，那么大小是多少呢，这里自己找下代码就好，其实就是cpu核数两倍，
        // 为什么是两倍呢，在创建NioEvetLoop的时候，默认也是这个值，也就是这样的话每个线程都有独享Area
        final PoolArena<byte[]> heapArena = leastUsedArena(heapArenas);
        final PoolArena<ByteBuffer> directArena = leastUsedArena(directArenas);

        final Thread current = Thread.currentThread();
        //useCacheForAllThreads 这个默认是true的
        if (useCacheForAllThreads || current instanceof FastThreadLocalThread) {
            // 创建这个cache
            final PoolThreadCache cache = new PoolThreadCache(
                    heapArena, directArena, tinyCacheSize, smallCacheSize, normalCacheSize,
                    DEFAULT_MAX_CACHED_BUFFER_CAPACITY, DEFAULT_CACHE_TRIM_INTERVAL);

            if (DEFAULT_CACHE_TRIM_INTERVAL_MILLIS > 0) {
                final EventExecutor executor = ThreadExecutorMap.currentExecutor();
                if (executor != null) {
                    executor.scheduleAtFixedRate(trimTask, DEFAULT_CACHE_TRIM_INTERVAL_MILLIS,
                            DEFAULT_CACHE_TRIM_INTERVAL_MILLIS, TimeUnit.MILLISECONDS);
                }
            }
            return cache;
        }
        // No caching so just use 0 as sizes.
        return new PoolThreadCache(heapArena, directArena, 0, 0, 0, 0, 0);
    }
```

从这里应该也可以看出，netty是如何减少多线程内存分配之间的竞争

**2：在线程局部缓存的Area上进行内存分配**

​	里面的实现细节，我们后面再分析。

* Heap分配

  `io.netty.buffer.PooledByteBufAllocator#newHeapBuffer`

  ```java
   @Override
      protected ByteBuf newHeapBuffer(int initialCapacity, int maxCapacity) {
          // 获取局部缓存
          PoolThreadCache cache = threadCache.get();
          // 获取缓存中的Area
          PoolArena<byte[]> heapArena = cache.heapArena;
  
          final ByteBuf buf;
          if (heapArena != null) {
              //分配
              buf = heapArena.allocate(cache, initialCapacity, maxCapacity);
          } else {
              buf = PlatformDependent.hasUnsafe() ?
                      new UnpooledUnsafeHeapByteBuf(this, initialCapacity, maxCapacity) :
                      new UnpooledHeapByteBuf(this, initialCapacity, maxCapacity);
          }
  
          return toLeakAwareBuffer(buf);
      }
  
  ```

  

* Direct的分配

  `io.netty.buffer.PooledByteBufAllocator#newDirectBuffer`

```java
	 @Override
    protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {
        //首先是拿到这个threadCache对象，相比之前应该有了解过ThreadLocal，也就是线程局部缓存
        // 其实netty 有自己实现一个FastThreadLocal ,这个是在构造中初始化的
        PoolThreadCache cache = threadCache.get();
        //再拿到directArena
        PoolArena<ByteBuffer> directArena = cache.directArena;

        final ByteBuf buf;
        if (directArena != null) {
            // 这里就是分配了（这里后面再分析吧）
            buf = directArena.allocate(cache, initialCapacity, maxCapacity);
        } else {
            buf = PlatformDependent.hasUnsafe() ?
                    UnsafeByteBufUtil.newUnsafeDirectByteBuf(this, initialCapacity, maxCapacity) :
                    new UnpooledDirectByteBuf(this, initialCapacity, maxCapacity);
        }

        return toLeakAwareBuffer(buf);
    }
```



### 内存分配

上面已经分析内存分配器，其实主要就上面两种，upooled来说是相对比较简单和容易理解的，对于pooled Netty的实现就比较复杂，因为他做了很多优化，比如:避免内存创建的消耗等，用到一些算法和策略。这里先了解一些基础变量和类。

```java
 	private static final int DEFAULT_TINY_CACHE_SIZE;//tinyCacheSize
    private static final int DEFAULT_SMALL_CACHE_SIZE;//smallCacheSize
    private static final int DEFAULT_NORMAL_CACHE_SIZE;//normalCacheSize
	
	//对应的默认值
DEFAULT_TINY_CACHE_SIZE = SystemPropertyUtil.getInt("io.netty.allocator.tinyCacheSize", 512);
        DEFAULT_SMALL_CACHE_SIZE = SystemPropertyUtil.getInt("io.netty.allocator.smallCacheSize", 256);
        DEFAULT_NORMAL_CACHE_SIZE = SystemPropertyUtil.getInt("io.netty.allocator.normalCacheSize", 64);
	
	// pageSize 
	private static final int DEFAULT_PAGE_SIZE;
	 int defaultPageSize = SystemPropertyUtil.getInt("io.netty.allocator.pageSize", 8192);// 默认大小是8K
        Throwable pageSizeFallbackCause = null;
        try {
            validateAndCalculatePageShifts(defaultPageSize);
        } catch (Throwable t) {
            pageSizeFallbackCause = t;
            defaultPageSize = 8192;
        }
        DEFAULT_PAGE_SIZE = defaultPageSize;


 private static final int DEFAULT_MAX_ORDER; // 8192 << 11 = 16 MiB per chunk
// chuckSize 默认16M ,就是上面公式计算出来的。
final int defaultChunkSize = DEFAULT_PAGE_SIZE << DEFAULT_MAX_ORDER;
//当然这个地方也是去计算chuckSize的
io.netty.buffer.PooledByteBufAllocator#validateAndCalculateChunkSize
```

大概的流程

>1. new一个ByteBuf，这里是分析pooled
>2. 从缓存中查找，没有可用的缓存进行下一步
>3. 从内存池中查找可用的内存，查找的方式如上所述（tiny、small、normal）
>4. 如果找不到则重新申请内存，并将申请到的内存放入内存池
>5. 使用申请到的内存初始化ByteBuf

首先，要了解的是，在pooled中，对于heap和direct都有一个自己的对象池。可以看下代码

```java
 private static final ObjectPool<PooledUnsafeHeapByteBuf> RECYCLER = ObjectPool.newPool(
            new ObjectCreator<PooledUnsafeHeapByteBuf>() {
        @Override
        public PooledUnsafeHeapByteBuf newObject(Handle<PooledUnsafeHeapByteBuf> handle) {
            return new PooledUnsafeHeapByteBuf(handle, 0);
        }
    });

 private static final ObjectPool<PooledUnsafeDirectByteBuf> RECYCLER = ObjectPool.newPool(
            new ObjectCreator<PooledUnsafeDirectByteBuf>() {
        @Override
        public PooledUnsafeDirectByteBuf newObject(Handle<PooledUnsafeDirectByteBuf> handle) {
            return new PooledUnsafeDirectByteBuf(handle, 0);
        }
    });

// 看源码应该知道 这个ObjectPool 也就是 RECYCLER是 RecyclerObjectPool
	 public static <T> ObjectPool<T> newPool(final ObjectCreator<T> creator) {
        return new RecyclerObjectPool<T>(ObjectUtil.checkNotNull(creator, "creator"));
    }
//当在获取对象池的时候，可以直接RECYCLER.get()，如果有这个对象的对象池，就会直接返回，否则就新建一个。
// 这里面的逻辑 具体还是要自己跟一下源码 这里只看下get方法。
 @SuppressWarnings("unchecked")
    public final T get() {
        if (maxCapacityPerThread == 0) {
            return newObject((Handle<T>) NOOP_HANDLE);
        }
        // 这其实也是线程的一个局部缓存
        Stack<T> stack = threadLocal.get();
       
        DefaultHandle<T> handle = stack.pop();
        if (handle == null) {
             //这个handler 就是用来负责回收的
            handle = stack.newHandle();
            // 把这个handler传进去 这个newObject 就是自己实现的方法，会返回具体的buf
            handle.value = newObject(handle);
        }
        return (T) handle.value;
    }
```



#### directArena分配

这个主要是用来先看下这个Arena的分配流程是如何的，上面已经上过了，pageSize,和chuckSize的大小。

1：在ThreadCache上拿到directArena

 `io.netty.buffer.PooledByteBufAllocator#newDirectBuffer`

```java
tected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {
        PoolThreadCache cache = threadCache.get();
        PoolArena<ByteBuffer> directArena = cache.directArena;

```

2: 获取bytebuf

`io.netty.buffer.PoolArena#allocate(io.netty.buffer.PoolThreadCache, int, int)`

```java
 PooledByteBuf<T> allocate(PoolThreadCache cache, int reqCapacity, int maxCapacity) {
     	//这里我们看这个方法
        PooledByteBuf<T> buf = newByteBuf(maxCapacity);
        allocate(cache, buf, reqCapacity);
        return buf;
    }

 	@Override
        protected PooledByteBuf<ByteBuffer> newByteBuf(int maxCapacity) {
            if (HAS_UNSAFE) {
                // 接着看这个
                return PooledUnsafeDirectByteBuf.newInstance(maxCapacity);
            } else {
                return PooledDirectByteBuf.newInstance(maxCapacity);
            }
        }

 //io.netty.buffer.PooledUnsafeDirectByteBuf#newInstance
 static PooledUnsafeDirectByteBuf newInstance(int maxCapacity) {
     	// RECYCLER 这个就是带有回收的对象次，也就是说，如果这个对像池中有就直接返回。
     	// 否则就创建一个
        PooledUnsafeDirectByteBuf buf = RECYCLER.get();
     	// 设置引用 和索引
        buf.reuse(maxCapacity);
        return buf;
    }
```

3: 分配

`io.netty.buffer.PoolArena#allocate(io.netty.buffer.PoolThreadCache, io.netty.buffer.PooledByteBuf<T>, int)`，这里的代码有点长，其实也是核心的地方，但后面会具体分析。

```java
private void allocate(PoolThreadCache cache, PooledByteBuf<T> buf, final int reqCapacity) {
        final int normCapacity = normalizeCapacity(reqCapacity);
        if (isTinyOrSmall(normCapacity)) { // capacity < pageSize
            int tableIdx;
            PoolSubpage<T>[] table;
            boolean tiny = isTiny(normCapacity);
            if (tiny) { // < 512
                if (cache.allocateTiny(this, buf, reqCapacity, normCapacity)) {
                    // was able to allocate out of the cache so move on
                    return;
                }
                tableIdx = tinyIdx(normCapacity);
                table = tinySubpagePools;
            } else {
                if (cache.allocateSmall(this, buf, reqCapacity, normCapacity)) {
                    // was able to allocate out of the cache so move on
                    return;
                }
                tableIdx = smallIdx(normCapacity);
                table = smallSubpagePools;
            }

            final PoolSubpage<T> head = table[tableIdx];

            synchronized (head) {
                final PoolSubpage<T> s = head.next;
                if (s != head) {
                    assert s.doNotDestroy && s.elemSize == normCapacity;
                    long handle = s.allocate();
                    assert handle >= 0;
                    s.chunk.initBufWithSubpage(buf, null, handle, reqCapacity, cache);
                    incTinySmallAllocation(tiny);
                    return;
                }
            }
            synchronized (this) {
                allocateNormal(buf, reqCapacity, normCapacity, cache);
            }

            incTinySmallAllocation(tiny);
            return;
        }
    //如果这里大于page 小于chuck 那就是page级别的分配 后面再专门分析这个吧
        if (normCapacity <= chunkSize) {	
            if (cache.allocateNormal(this, buf, reqCapacity, normCapacity)) {
                // was able to allocate out of the cache so move on
                return;
            }
            synchronized (this) {
                allocateNormal(buf, reqCapacity, normCapacity, cache);
                ++allocationsNormal;
            }
        } else {
            // Huge allocations are never served via the cache so just call allocateHuge
            allocateHuge(buf, reqCapacity);
        }
    }
```

上面看到`cache.allocateXXX`就是表明是从缓存上进行分配，这里先知道就好，分配成功就返回，否则，就会内存堆分配。如果需要分配的大小大于16M，就会allocateHuge()来分配，也不是在缓存上。上面就是一个内存分配的流程，后面要分析的肯定是和缓存有关了。

#### 内存规格

先看图说明

![bytebuf3](https://github.com/tryingpfq/tryingpfq.github.io/blob/master/picture/bytebuf3.jpg?raw=true)

内存申请是以chuck为单位向操作系统申请，也就是16M，后续的所有的内存分配都是在chuck中分配。比如需要分配一个1M内存，首先需要向操作系统申请一个chuck,然后在这个chuck中，取一段1M大小的内存进行分配，放到bytebuf中。但这样又个问题，假如有些内存，比较小，16M就会比较大，所以用chuck进行切分成Page,也就是2^11=2028个page，比如这个是否分配16K大小的内存，就只需要两个连续page就行。从0---8K也有一个单位就是Subpage,比如以8k为单位进行分配的，在申请比较小的，比如20字节，以page分配的话，就会很浪费。假如分配2K内存，就只需要把Page切分4分，然后在这个page中取取一段就好，同理如果是512B，就分成16个片段就好。这里就是netty中内存规格的一个介绍。



#### 基本数据结构

#### PoolThreadCache

这个其实就是线程的一个局部缓存。里面主要会有六个缓存，其实是三种。

```java
private final MemoryRegionCache<byte[]>[] tinySubPageHeapCaches;
private final MemoryRegionCache<byte[]>[] smallSubPageHeapCaches;
private final MemoryRegionCache<ByteBuffer>[] tinySubPageDirectCaches;
private final MemoryRegionCache<ByteBuffer>[] smallSubPageDirectCaches;
private final MemoryRegionCache<byte[]>[] normalHeapCaches;
private final MemoryRegionCache<ByteBuffer>[] normalDirectCaches;
//看名字应该就知道意思
// 接下来看下MemoryRegionCache 的内容
private abstract static class MemoryRegionCache<T> {
        private final int size;
        private final Queue<Entry<T>> queue;
        private final SizeClass sizeClass;
        private int allocations;

        MemoryRegionCache(int size, SizeClass sizeClass) {
            this.size = MathUtil.safeFindNextPositivePowerOfTwo(size);
            queue = PlatformDependent.newFixedMpscQueue(this.size);
            this.sizeClass = sizeClass;
        }
}
// 看下创建
 private static <T> MemoryRegionCache<T>[] createSubPageCaches(
            int cacheSize, int numCaches, SizeClass sizeClass) {
        if (cacheSize > 0 && numCaches > 0) {
            @SuppressWarnings("unchecked")
            MemoryRegionCache<T>[] cache = new MemoryRegionCache[numCaches];
            /**
             * 对于tiny来说，cacheSize的默认值是512，numCaches是512 >>> 4 = 32
             * 对于Small来说，cacheSize的默认值是256，cacheSize和pageShift有关-pageSize 这里			 * samll的numcaches= 4;
             */
            for (int i = 0; i < cache.length; i++) {
                cache[i] = new SubPageMemoryRegionCache<T>(cacheSize, sizeClass);
            }
            return cache;
        } else {
            return null;
        }
    }
```

看下MemoryRegionCache，我们可以看下下面两张图

可以看下这中图

![bytebuf5](https://github.com/tryingpfq/tryingpfq.github.io/blob/master/picture/bytebuf5.jpg?raw=true)

其实就是三种类型，tiny,samll,normal,然后各自有个队列，大小。

再看下这张图，描述节点的大小。

![bytebuf6](https://github.com/tryingpfq/tryingpfq.github.io/blob/master/picture/bytebuf6.jpg?raw=true)

可见第一个节点是不存储数据的，比如tiny，会有32个节点，tiny[1]表示16B的一个缓存的Queue，tiny[2]表示32B,对于tiny来说，每个节点中queue的大小为512.



#### PoolArena

oolArena负责缓存池的全局调度，它负责在上层组织和管理所有的chunk以及subpage单元。为了减少多线程请求内存池时的同步处理，Netty默认提供了cpu核数*2个PoolArena示例，上面我们有提到这个地方。

> We use 2 *available processors by default to reduce contention as we use 2* available processors for the number of EventLoops in NIO and EPOLL as well. If we choose a smaller number we will run into hot spots as allocation and de-allocation needs to be synchronized on the PoolArena.

每个PoolArena管理的所有chunk根据内存使用率的不同被划分为6种类型，以双向链表ChunkList的方式组织，并在不断的内存分配过程中根据chunk的使用率，对chunk的类型进行调整，放入合适的链表中。

先看PoolArena中的PoolChunkList，在初始化PoolArena的时候，会初始化6中类型的ChuckList，每个双向链表chuckLIst中的节点是PoolChuck。这里还是跟着代码来看吧。

```java
protected PoolArena(PooledByteBufAllocator parent, int pageSize,
          int maxOrder, int pageShifts, int chunkSize, int cacheAlignment) {
      	
    	//使用率100  这个构造看下面贴的代码，这里只要明白每个poolChuckList创建的时候，传的参数具体是
    	// 什么内容
    	// 要注意的是，链表直接也是可以互相引用的，从这里看 q100 的next是为null的
        q100 = new PoolChunkList<T>(this, null, 100, Integer.MAX_VALUE, chunkSize);
        //75 使用范围是75-100 q75的next是 q100 ，后面的也是类似，就不再累赘了
    	q075 = new PoolChunkList<T>(this, q100, 75, 100, chunkSize);
        q050 = new PoolChunkList<T>(this, q075, 50, 100, chunkSize);
        q025 = new PoolChunkList<T>(this, q050, 25, 75, chunkSize);
        q000 = new PoolChunkList<T>(this, q025, 1, 50, chunkSize);
        qInit = new PoolChunkList<T>(this, q000, Integer.MIN_VALUE, 25, chunkSize);

    	// 设置前置节点
        q100.prevList(q075);
        q075.prevList(q050);
        q050.prevList(q025);
        q025.prevList(q000);
        q000.prevList(null);
        qInit.prevList(qInit);
    }

PoolChunkList(PoolArena<T> arena, PoolChunkList<T> nextList, int minUsage, int maxUsage, int chunkSize) {
        assert minUsage <= maxUsage;
        this.arena = arena;
        this.nextList = nextList;
        this.minUsage = minUsage;
        this.maxUsage = maxUsage;
        maxCapacity = calculateMaxCapacity(minUsage, chunkSize);

        freeMinThreshold = (maxUsage == 100) ? 0 : (int) (chunkSize * (100.0 - maxUsage + 0.99999999) / 100L);
        freeMaxThreshold = (minUsage == 100) ? 0 : (int) (chunkSize * (100.0 - minUsage + 0.99999999) / 100L);
    }

```

然后看下这个几个链表的一个结构图

![bytebuf7](https://github.com/tryingpfq/tryingpfq.github.io/blob/master/picture/bytebuf7.png?raw=true)

* allocate流程

  这里就表明一下是哪个方法，具体就不点进去看了，后面专门对这块流程会进行分析。

  前面已经说到过，对于Poole内存管理器分配的时候，首先是拿到ThreadCache，然后拿到Arena(直接或者堆)，然后在这个arena上去进行分配。

  `io.netty.buffer.PoolArena#allocate(io.netty.buffer.PoolThreadCache, int, int)`

  `io.netty.buffer.PoolArena#allocate(io.netty.buffer.PoolThreadCache, io.netty.buffer.PooledByteBuf<T>, int)`

  后面再把整个流程屡一下吧，大概流程就是这样的

  >1. 对请求的内存大小做规整化处理：大于chunkSize时调整为保持内存对齐即可；大于512时调整为大于等于请求值的2的最小幂；小于512调整为大于等于请求值的16的倍数
  >2. 当请求内存小于pageSize时，在页内分配：请求内存小于512在tinysubpages数组中定位并执行分配，大于512时在smallpages数组中定位并执行分配，当数组中没有可用的内存页时，在chunk中寻找新的内存页进行分配。
  >3. 请求内存大于pageSize小于chunkSize，在chunk内执行分配
  >4. 请求内存大于chunkSize，直接调用allocateHuge分配，不在内存池中管理。

  

#### poolChuck

前面已经说到过，操作系统是以chuck为单位向操作系统申请内存的，然后chuck是page的单元集合，并且一个chuck的大小上面也说过了，默认大小是16M。

```java
  PoolChunkList<T> parent;
  PoolChunk<T> prev;
  PoolChunk<T> next;

PoolChunk(PoolArena<T> arena, T memory, int pageSize, int maxOrder, int pageShifts, int chunkSize, int offset) {
        unpooled = false;
        this.arena = arena;
        this.memory = memory;
        this.pageSize = pageSize;
        this.pageShifts = pageShifts;
        this.maxOrder = maxOrder;// 这个默认值是11 也就是这可二叉树的深度
        this.chunkSize = chunkSize;
        this.offset = offset;
        unusable = (byte) (maxOrder + 1); //表示超过深度 就不可用
        log2ChunkSize = log2(chunkSize);
        subpageOverflowMask = ~(pageSize - 1);
        freeBytes = chunkSize;

        assert maxOrder < 30 : "maxOrder should be < 30, but is: " + maxOrder;
        maxSubpageAllocs = 1 << maxOrder;

        // Generate the memory map. 
    	// 这里后面再分析这个吧 其实就是初始化第几个节点在第几层
        memoryMap = new byte[maxSubpageAllocs << 1];
        depthMap = new byte[memoryMap.length];
        int memoryMapIndex = 1;
        for (int d = 0; d <= maxOrder; ++ d) { // move down the tree one level at a time
            int depth = 1 << d; // 表示这层有多少节点
            for (int p = 0; p < depth; ++ p) {
                // memoryMap 记录节点的当前深度 这里可以先的图
                memoryMap[memoryMapIndex] = (byte) d;
                depthMap[memoryMapIndex] = (byte) d;
                memoryMapIndex ++;
            }
        }
        subpages = newSubpageArray(maxSubpageAllocs);
        cachedNioBuffers = new ArrayDeque<ByteBuffer>(8);
    }
```

这里有必要详细说明一下:

1：chuck内存块，netty向操作系统申请资源的基本单位。默认大小是16M

​	ChunkSize = pageSize*2maxOrder   根据默认值，算的是16M

​	MaxChunkSize，chuck最大为1G

​	DefaultMaxOrder  = 11,一个chunk默认由211个页面构成，因为一个page 8k，所以默认完全二叉树11层。

2: 内存页Page，当请求的内存小于页大小时，可继续划分为更小的内存段(subPage)。page大小默认为8k

3: Netty使用了一个数组memoryMap来表示这个完全二叉树。

先看下这个图的标示：



![bytebuf4](https://github.com/tryingpfq/tryingpfq.github.io/blob/master/picture/bytebuf4.png?raw=true)



详细的来说，数组下标表示二叉树中各节点的编号id，数组元素内容表示当前节点可分配内存的子节点（即未分配）在二叉树中的深度。根据i节点在memoryMap中的取值不同，它可以有一下三种语义：

1. memoryMap[i] = depth_of_i 当前节点及其所有子节点都可以用来分配
2. memoryMap[i] = x > depth_of_i 至少有一个子节点被分配，无法直接使用此节点，但其在第x层的子节点中有可分配的内存
3. memoryMap[i] = maxOrder + 1 当前节点的所有子节点均已被分配

以上图节点3为例，当memoryMap[3]=1时，表示该节点及其子节点均可分配，memoryMap[3]=2时，表示节点6和7中至少有一个已经被分配，并且在这两个节点中还能找到未分配的空间，memoryMap[3]=4时，表示该节点下的所有空间均已经被分配，无法再使用。

* **page级别的分配流程**

  `io.netty.buffer.PoolChunk#allocateRun`

  ```java
  /**
   * 向PoolChunk申请一段内存
   * /
  long allocate(int normCapacity)  {
      if ((normCapacity & subpageOverflowMask) != 0) { // >= pageSize 8k
      	//当要分配的内存大于pageSize时候，使用allocateRun，就不能在页内分配了
          return allocateRun(normCapacity); 
      } else {
      	//否则使用向PoolChunk申请一段内存在page内分配
          return allocateSubpage(normCapacity);
      }
  }
  
  //这里只分析chuck上的分配，page分配后面再分析
  // io.netty.buffer.PoolChunk#allocateRun
  private long allocateRun(int normCapacity) {
  	//再根据请求内存的大小，定位其在二叉树中的深度 也就是第几层 	
      int d = maxOrder - (log2(normCapacity) - pageShifts);
      int id = allocateNode(d);
      if (id < 0) {
          return id;
      }
      freeBytes -= runLength(id);
      return id;
  }
  
  ```

  >上面定位在二叉树的深度，有两种方式
  >
  >1. 自底向上：父节点的内存是子节点的二倍，比子节点高一层；父节点的内存是孙子节点的四倍，比孙子节点高两层，所以拥有normalCapacity内存的节点应该比叶子节点高：log2(normalCapacity/pagesie) = log2(normalCapacity)-pageShifts层，也就是说它在树中的深度应该为maxOrder-(log2(normalCapacity)-pageShifts).
  >2. 自顶向下：观察上图右侧说明信息的第三列，根节点拥有整个chunk的内存，任意d层节点拥有的内存Capacity=chunksize／2d, 两边取对数可得拥有normalCapacity内存的节点应该在log2(chunksize/normalCapacity)这一层上。

* 如何在memoryMap的d层上寻找第一个可分配的内存节点。同样先看代码

  ```java
  private int allocateNode(int d) {
          int id = 1;// 也就是从根节点开始寻找
          int initial = - (1 << d); // has last d bits = 0 and rest all = 1
          byte val = value(id);
          if (val > d) { // unusable 根节点的容量不足以满足要分配的内存大小
              return -1;
          }
          while (val < d || (id & initial) == 0) { // id & initial == 1 << d for all ids at depth d, for < d it is 0
              id <<= 1; //取左子节点
              val = value(id);
              if (val > d) {
                  id ^= 1; //取邻居节点（2->3, 3->2）
                  val = value(id);
              }
          }
          byte value = value(id);
          assert value == d && (id & initial) == 1 << d : String.format("val = %d, id & initial = %d, d = %d",
                  value, id & initial, d);
          setValue(id, unusable); // mark as unusable
      	// 找到后，也对这颗二叉树进行更新，也就是逐层往上标记父节点被使用
          updateParentsAlloc(id);
          return id;
      }
  
  ```

  

  这里的核心思想就是，从根节点开始，如果根节点已经被分配，并且其可分配内存的子节点深度大于d，表示已没有足够且连续的内存空间用来分配，返回-1；

  如果左子节点上的内存可分配则在左子节点上分配，否则尝试右子节点，依次迭代。

  这里主要注意下循环的迭代条件：
  
  1. 如果当前节点可分配节点的深度小于目标深度（相应的内存也就大于请求的内存），说明子节点就能够满足条件，进入下一层；
  2. 当一个节点(id)可分配节点的深度与目标深度相等时，只要当前节点(id)的深度小于目标深度（id & initial == 0），就应该进入下一层迭代到目标深度那一层，寻找可用的节点。

#### PoolSubpage

当Netty分配内存大小小于page时候，Netty提供PoolSubpage把chunk的一个page节点8k内存划分成更小的内存段，通过对每个内存段的标记与清理标记进行内存的分配和释放。

subpage是针对请求内存小于一个页面大小时的内存划分。根据请求内存大小，subpage有可以被分为，smallpage(>=512 btyes)和tinypage(<512 bytes)两种类型。在内存分配的过程中，为了保持subpage中内存的连续性，避免内存碎片，并方便根据内存偏移量进行定位，每个页面内分配的内存段应该具有统一的规格(PoolSubpage中的elemSize)。这里的内容参考了: [PoolSubpage](https://gsmtoday.github.io/2017/08/27/poolsubpage/)

```java
// 看下字段和构造

    final PoolChunk<T> chunk;
    private final int memoryMapIdx;
    private final int runOffset;
    private final int pageSize;
    private final long[] bitmap;

    PoolSubpage<T> prev;
    PoolSubpage<T> next;

    boolean doNotDestroy;
    int elemSize;
    private int maxNumElems;
    private int bitmapLength;
    private int nextAvail;
    private int numAvail;

 PoolSubpage(PoolSubpage<T> head, PoolChunk<T> chunk, int memoryMapIdx, int runOffset, 			int pageSize, int elemSize) {
        this.chunk = chunk;
        this.memoryMapIdx = memoryMapIdx;
        this.runOffset = runOffset;
        this.pageSize = pageSize;
        bitmap = new long[pageSize >>> 10]; // pageSize / 16 / 64
        init(head, elemSize);
    }
```

PoolSubPage在页内进行内存分配，用位图记录内存分配的情况，位图标记为0表示未分配，标记为1表示已分配。我们前面已经提到过，pageSize=8K，所以位图bitmap默认大小初始化为8，在Page中subpage以16字节为最小单位划分内存段，而一个long类型的变量有64位，所以最多使用PageSize/16/64=8个
long型的变量就可以表示所有内存段。

假设我们以elemSize=72字节为单位，在页内进行内存段的划分，那最多将有maxNumElem=PageSize/elemSize=113个内存段。（elemSize一旦确定就不会改变， 页面中内存段都是大小一致的），那么这113个内存段就要占用位图中113个位置，那需要多少个bitmap元素呢？肯定是不需要的了，这里必须要了解位图的实现。这里位图中保存的元素是long型，一个long64为，那么就可以标示64个状态位，所以113个位置，只需要2个数组元素就ok。



* **subPage级别的分配流程**

  PoolSubPage分配内存段的过程就是在**位图中找到第一个未被使用的内存段**，返回一个描述其内存位置偏移量的整数句柄，用于定位。同样来看下代码的实现

  ```java
  //当用户请求的内存空间小于一个页面的内存大小时，会调用allocateSubpage在页面内进行内存分配。
  //io.netty.buffer.PoolChunk#allocateSubpage 
   private long allocateSubpage(int normCapacity) {
       	// 这个find方法要研究一下啊
          PoolSubpage<T> head = arena.findSubpagePoolHead(normCapacity);
          int d = maxOrder; // subpages are only be allocated from pages i.e., leaves
          synchronized (head) {
              int id = allocateNode(d);
              if (id < 0) {
                  return id;
              }
  
              final PoolSubpage<T>[] subpages = this.subpages;
              final int pageSize = this.pageSize;
  
              freeBytes -= pageSize;
  
              int subpageIdx = subpageIdx(id);
              PoolSubpage<T> subpage = subpages[subpageIdx];
              if (subpage == null) {
                  subpage = new PoolSubpage<T>(head, this, id, runOffset(id), pageSize, normCapacity);
                  subpages[subpageIdx] = subpage;
              } else {
                  subpage.init(head, normCapacity);
              }
              return subpage.allocate();
          }
      }
  
  
  // io.netty.buffer.PoolSubpage#allocate
  long allocate() {
          if (elemSize == 0) {
              return toHandle(0);
          }
  
          if (numAvail == 0 || !doNotDestroy) {
              return -1;
          }
  		// 看这个方法
      	//取数组元素的位置
          final int bitmapIdx = getNextAvail();
          int q = bitmapIdx >>> 6;
          int r = bitmapIdx & 63;
          assert (bitmap[q] >>> r & 1) == 0;
      	// 这里分配后，需要标志位1 以防再次分配
          bitmap[q] |= 1L << r;
  
          if (-- numAvail == 0) {
              removeFromPool();
          }
  		// 这里返回的是一个偏移量句柄
      	//还需要做一次变化（toHandle），其结构也比较明显，句柄共占据long型的低48位，其中低32位记录
     		//当前内存页在PoolChunk的平衡二叉树中的节点编号，中间16位（低6位记录在位图long型元素的二进		 //制位置，低3位记录在位图数组中的位置）。
          return toHandle(bitmapIdx);
      }
  
  // io.netty.buffer.PoolSubpage#findNextAvail 
  private int findNextAvail() {
          final long[] bitmap = this.bitmap;
          final int bitmapLength = this.bitmapLength;
          for (int i = 0; i < bitmapLength; i ++) {
              long bits = bitmap[i];
              // 表示这个内存段是没有被使用的
              if (~bits != 0) {
                  return findNextAvail0(i, bits);
              }
          }
          return -1;
      }
  
  /**
    * i ：空闲内存在位图数组中的下标
    * bits : 数组元素表示的位图详情
    * return ：位图索引
    */
   private int findNextAvail0(int i, long bits) {
          final int maxNumElems = this.maxNumElems;
          final int baseVal = i << 6;
  
          for (int j = 0; j < 64; j ++) {
              if ((bits & 1) == 0) {
                  int val = baseVal | j;
                  if (val < maxNumElems) {
                      return val;
                  } else {
                      break;
                  }
              }
              bits >>>= 1;
          }
          return -1;
      }
  ```

  算法首先在位图数组bitmap中开始遍历，如果当前数组元素表示的内存空间上有空闲内存段(即数组元素的二进制位上有0)，则进一步在此数组元素中查找空闲内存段在二进制位上的位置。通过在二进制位上循环移位遍历，访问到0则构造内存偏移量并返回。整形的内存偏移量，低6位用来表示空闲内存在long型元素的二进制位表示中占据的位置，高位用来记录该数组元素的下标。




#### 缓存命中流程

首先，这里我们以调试的方式，进行根据源代码，有些细节的地方，在上面已经提到过。

```java
 public static void main(String[] args) {
        PooledByteBufAllocator allocator = PooledByteBufAllocator.DEFAULT;
        ByteBuf byteBuf1 = allocator.directBuffer(510);
        ByteBuf byteBuf2 = allocator.directBuffer(1024);
    }
```

首先是获取线程局部缓存`PoolThreadCache`，在获取`directArena`，这里我们分析direct。

```java
 @Override
    protected ByteBuf newDirectBuffer(int initialCapacity, int maxCapacity) {
        PoolThreadCache cache = threadCache.get();
        PoolArena<ByteBuffer> directArena = cache.directArena;
        final ByteBuf buf;
        if (directArena != null) {
            buf = directArena.allocate(cache, initialCapacity, maxCapacity);
        } else {
            buf = PlatformDependent.hasUnsafe() ?
                    UnsafeByteBufUtil.newUnsafeDirectByteBuf(this, initialCapacity, maxCapacity) :
                    new UnpooledDirectByteBuf(this, initialCapacity, maxCapacity);
        }
        return toLeakAwareBuffer(buf);
    }
```

`io.netty.buffer.PoolArena#allocate(io.netty.buffer.PoolThreadCache, int, int)`，里面就不看了，之前有看过

`io.netty.buffer.PoolArena#allocate(io.netty.buffer.PoolThreadCache, io.netty.buffer.PooledByteBuf<T>, int)`这个方法应该是核心了。注意我们这里是假设缓存有的情况 

```java
// 注意下这里方法参数，cache是缓存，buf:是上面新建的，reqCapacity是要分配的内存大小 这里我们调试30字节
private void allocate(PoolThreadCache cache, PooledByteBuf<T> buf, final int reqCapacity) {
    	// 内存大小规格化 就是必修是2的幂数 这里先会判断这个大小区间，然后进行处理，里面方法就不看了
    	// 比如现在是30 就是属于tiny,就会做这个规格化 
    	//(reqCapacity & ~15) + 16 = 32  以16的倍数进行递增。
        final int normCapacity = normalizeCapacity(reqCapacity);
    	// 这个方法实现很简单，但要了解其用途，就是判断要分配的内存大小是否小于pageSize
        if (isTinyOrSmall(normCapacity)) { // capacity < pageSize  
            int tableIdx;
            PoolSubpage<T>[] table;
            //判断是否为tiny
            boolean tiny = isTiny(normCapacity);
            
            if (tiny) { // < 512
                // 就是在tiny上分配 这里的调用看后面tiny的分配
                if (cache.allocateTiny(this, buf, reqCapacity, normCapacity)) {
                    // was able to allocate out of the cache so move on
                    return;
                }
                // 接着 这里算出来的是2 在上面方法的里面有说明
                tableIdx = tinyIdx(normCapacity);
                table = tinySubpagePools;
            } else {
                // 否则 就是在small上分配
                if (cache.allocateSmall(this, buf, reqCapacity, normCapacity)) {
                    // was able to allocate out of the cache so move on
                    return;
                }
                tableIdx = smallIdx(normCapacity);
                table = smallSubpagePools;
            }
			// 
            final PoolSubpage<T> head = table[tableIdx];

            synchronized (head) {
                final PoolSubpage<T> s = head.next;
                if (s != head) {
                    assert s.doNotDestroy && s.elemSize == normCapacity;
                    long handle = s.allocate();
                    assert handle >= 0;
                    s.chunk.initBufWithSubpage(buf, null, handle, reqCapacity, cache);
                    incTinySmallAllocation(tiny);
                    return;
                }
            }
            synchronized (this) {
                allocateNormal(buf, reqCapacity, normCapacity, cache);
            }

            incTinySmallAllocation(tiny);
            return;
        }
        if (normCapacity <= chunkSize) {
            if (cache.allocateNormal(this, buf, reqCapacity, normCapacity)) {
                // was able to allocate out of the cache so move on
                return;
            }
            synchronized (this) {
                allocateNormal(buf, reqCapacity, normCapacity, cache);
                ++allocationsNormal;
            }
        } else {
            // Huge allocations are never served via the cache so just call allocateHuge
            allocateHuge(buf, reqCapacity);
        }
    }
```

* tiny上分配

  这里的大概流程：

  > 1：找到对应大小size的MemoryRegionCache
  >
  > 2：从queue中弹出一个entry给ByteBuf初始化，这个entry就是指某个chuck下，连续的一段内存。
  >
  > 3：将弹出的entry扔到对象池中进行复用(通过recycle进行)

  

  `io.netty.buffer.PoolThreadCache#allocateTiny`

  ```java
   boolean allocateTiny(PoolArena<?> area, PooledByteBuf<?> buf, int reqCapacity, int normCapacity) {
          return allocate(cacheForTiny(area, normCapacity), buf, reqCapacity);
      }
  // 这里就是获取输入那个索引 开始默认的时候 tinySubPageDirectCaches 的长度是32的
   private MemoryRegionCache<?> cacheForTiny(PoolArena<?> area, int normCapacity) {
       	// tinyIndex 为啥是 normCapacity >>> 4  为什么除16呢 因为tiny中，一个节点的大小是16
       	// 很明显 因为这里是32 ，所以算出来的索引位置是2注意 因为是指一个区间长度，所以是2:0-1 1-2
          int idx = PoolArena.tinyIdx(normCapacity);
          if (area.isDirect()) {
              // 接下来是这里，然后又返回当上面的方法中 返回这个节点
              return cache(tinySubPageDirectCaches, idx);
          }
          return cache(tinySubPageHeapCaches, idx);
      }
  
  // 上面已经拿到了节点，这个方法就是在这个节点中，弹出一个Entry给bytebuf，记得把，对于tiny queue
  // 的大小是512
   public final boolean allocate(PooledByteBuf<T> buf, int reqCapacity, PoolThreadCache threadCache) {
              Entry<T> entry = queue.poll();
              if (entry == null) {
                  return false;
              }
       		// 接下来就是初始化了  entry中的chuck和handler 可以唯一定位chuck中的一块连续内存
              initBuf(entry.chunk, entry.nioBuffer, entry.handle, buf, reqCapacity, threadCache);
              entry.recycle();
  
              ++ allocations;
              return true;
          }
  
  //初始化 看下这个调用链的代码
   @Override
          protected void initBuf(
                  PoolChunk<T> chunk, ByteBuffer nioBuffer, long handle, PooledByteBuf<T> buf, int reqCapacity,
                  PoolThreadCache threadCache) {
              chunk.initBufWithSubpage(buf, nioBuffer, handle, reqCapacity, threadCache);
          }
  
    private void initBufWithSubpage(PooledByteBuf<T> buf, ByteBuffer nioBuffer,
                                      long handle, int bitmapIdx, int reqCapacity, PoolThreadCache threadCache) {
          assert bitmapIdx != 0;
  
          int memoryMapIdx = memoryMapIdx(handle);
  
          PoolSubpage<T> subpage = subpages[subpageIdx(memoryMapIdx)];
          assert subpage.doNotDestroy;
          assert reqCapacity <= subpage.elemSize;
  		// init
          buf.init(
              this, nioBuffer, handle,
              runOffset(memoryMapIdx) + (bitmapIdx & 0x3FFFFFFF) * subpage.elemSize + offset,
                  reqCapacity, subpage.elemSize, threadCache);
      }
  
    private void init0(PoolChunk<T> chunk, ByteBuffer nioBuffer,
                         long handle, int offset, int length, int maxLength, PoolThreadCache cache) {
          assert handle >= 0;
          assert chunk != null;
  		// 首先 表明是属于哪个chuck
          this.chunk = chunk;
          memory = chunk.memory;
          tmpNioBuf = nioBuffer;
          allocator = chunk.arena.parent;
          this.cache = cache;
        	// 这个handler 就是在这个chuck上的唯一标识
          this.handle = handle;
          this.offset = offset;
          this.length = length;
          this.maxLength = maxLength;
      }
  
  //在弹出entry的时候，就没有在用到的话，很用可能就会被GC掉，netty为了能够对象的复用，就把这个
  //entry 方法对象池中去。
  io.netty.buffer.PoolThreadCache.MemoryRegionCache.Entry#recycle
      void recycle() {
      chunk = null; 
      nioBuffer = null;
      handle = -1;
      recyclerHandle.recycle(this);
  }
  
  @Override
  public void recycle(Object object) {
      if (object != value) {
          throw new IllegalArgumentException("object does not belong to handle");
      }
  
      Stack<?> stack = this.stack;
      if (lastRecycledId != recycleId || stack == null) {
          throw new IllegalStateException("recycled already");
      }
  
      stack.push(this);
  }
  
  ```

  不过要注意点是，如果在创建bytebuf的时候，是不会有缓存的。



#### Page级别的分配

也就是说上面看分配代码的时候提到的一个地方,上面是就要缓存命中流程，但一开始是没有缓存的

```java
    //如果这里大于page小于chuck 那就是page级别的分配 后面再专门分析这个吧
        if (normCapacity <= chunkSize) {	
            if (cache.allocateNormal(this, buf, reqCapacity, normCapacity)) {
                // was able to allocate out of the cache so move on
                return;
            }
            synchronized (this) {
                allocateNormal(buf, reqCapacity, normCapacity, cache);
                ++allocationsNormal;
            }
        }
```

`io.netty.buffer.PoolArena#allocateNormal`,这个就是page基本的分配，也就是说，要么分配1page，要么2个，但不可能在一个page内部分配。当然也可能是开始没有命中缓存的时候，会调用这个方法

大概流程：

1：尝试在现有的chuck上分配，也就是开始看的6个chuckList中去寻找，但开始是没有的

2：创建一个chuck进行内存分配

```java
 // Method must be called inside synchronized(this) { ... } block
    private void allocateNormal(PooledByteBuf<T> buf, int reqCapacity, int normCapacity, PoolThreadCache threadCache) {
        // 开始是寻找现有的
        if (q050.allocate(buf, reqCapacity, normCapacity, threadCache) ||
            q025.allocate(buf, reqCapacity, normCapacity, threadCache) ||
            q000.allocate(buf, reqCapacity, normCapacity, threadCache) ||
            qInit.allocate(buf, reqCapacity, normCapacity, threadCache) ||
            q075.allocate(buf, reqCapacity, normCapacity, threadCache)) {
            return;
        }

        // Add a new chunk.
        PoolChunk<T> c = newChunk(pageSize, maxOrder, pageShifts, chunkSize);
        boolean success = c.allocate(buf, reqCapacity, normCapacity, threadCache);
        assert success;
        qInit.add(c);
    }

 boolean allocate(PooledByteBuf<T> buf, int reqCapacity, int normCapacity, PoolThreadCache threadCache) {
        final long handle;
        if ((normCapacity & subpageOverflowMask) != 0) { // >= pageSize
            // 这里的逻辑在上面有说到
            handle =  allocateRun(normCapacity);
        } else {
            handle = allocateSubpage(normCapacity);
        }

        if (handle < 0) {
            return false;
        }
        ByteBuffer nioBuffer = cachedNioBuffers != null ? cachedNioBuffers.pollLast() : null;
     	// 看下init这个方法
        initBuf(buf, nioBuffer, handle, reqCapacity, threadCache);
        return true;
    }
 void initBuf(PooledByteBuf<T> buf, ByteBuffer nioBuffer, long handle, int reqCapacity,
                 PoolThreadCache threadCache) {
        int memoryMapIdx = memoryMapIdx(handle);
        int bitmapIdx = bitmapIdx(handle);
        if (bitmapIdx == 0) {
            byte val = value(memoryMapIdx);
            assert val == unusable : String.valueOf(val);
            // 其实是想说这个runOffset，这里的mapId 其实就是在这个chuck的二叉树的一个节点位置
            // 对于这个buf的范围，要计算好初始位置，毕竟这个memory是这个chuck共享的。
            buf.init(this, nioBuffer, handle, runOffset(memoryMapIdx) + offset,
                    reqCapacity, runLength(memoryMapIdx), threadCache);
        } else {
            initBufWithSubpage(buf, nioBuffer, handle, bitmapIdx, reqCapacity, threadCache);
        }
    }

```



### 内存释放过程

先找到释放的入口：`io.netty.buffer.PooledByteBuf#deallocate`

```java
// 最终会调用这个方法，但是在之前有个判断，就是有没有被引用
public final boolean release(T instance) {
        int rawCnt = nonVolatileRawCnt(instance);
        return rawCnt == 2 ? tryFinalRelease0(instance, 2) || retryRelease0(instance, 1)
                : nonFinalRelease0(instance, 1, rawCnt, toLiveRealRefCnt(rawCnt, 1));
}

@Override
protected final void deallocate() {
    if (handle >= 0) {
        final long handle = this.handle;
        this.handle = -1;// 表示不在指向任何一块内存
        memory = null;
        //释放
        chunk.arena.free(chunk, tmpNioBuf, handle, maxLength, cache);
        tmpNioBuf = null;
        chunk = null;
        // 加到对象池
        recycle();
    }
}

 void free(PoolChunk<T> chunk, ByteBuffer nioBuffer, long handle, int normCapacity, PoolThreadCache cache) {
        if (chunk.unpooled) {
            int size = chunk.chunkSize();
            destroyChunk(chunk);
            activeBytesHuge.add(-size);
            deallocationsHuge.increment();
        } else {
            // 计算是那种size
            SizeClass sizeClass = sizeClass(normCapacity);
            //尝试加到缓存 根据不同规格，到不同数组中去找
            if (cache != null && cache.add(this, chunk, nioBuffer, handle, normCapacity, sizeClass)) {	
                // 加成功
                // cached so not free it.
                return;
            }
			//释放内存
            freeChunk(chunk, handle, sizeClass, nioBuffer, false);
        }
    }
```

释放流程：

> 1:连续的内存区段加到缓存，如果成功,进入第3，否则继续后面2
>
> 2：标记连续的内存区段为未使用
>
> ​	page是根据层标记是否可用，subPage用位图来表示是否可用
>
> 3：byteBuf加到对象池



### 总结

netty内存这块还是很复杂的，上面有些地方可能不一定写对了，有些细节的地方实现，我暂时也还不懂，以后赢要回来再看一遍。



以上部分内容参考: [闪电侠netty视频](https://coding.imooc.com/class/230.html)        [netty内存管理](https://gsmtoday.github.io/2017/08/27/poolchunk/)

