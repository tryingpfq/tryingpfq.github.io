---

layout:     post
title:      zookeeper
subtitle:   zookeeper源码分析
date:       2020-03-30
author:     tryingpfq
header-img: img/post-bg-zookeeper1.jpg
catalog: true
tags:
    - zookeeper
---


> 在看分布式有关的东西，然后就有用到zookeeper，觉得好奇，就打算把源码看看。
>

看这篇之前，如果想先对zk有个感性认识，可以看下[上一篇](http://tryingpfq.club/2019/07/14/zookeeper/)，本文主要是基于自己看源码时候的理解，当然可能有些地方理解有误，还有很多没有看懂的地方。但看我这篇，多少会有些理解，比如`启动过程` `选举过程` `集群节点通信` `客户端连接` `集群节点数据同步`

### 源码构建

这里就不多说了，参考：[zookeeper源码构建](https://segmentfault.com/a/1190000021451833)


### ZK集群模式启动过程

1：启动类：QuorumPeerMain（要设置main方法参数，就在zoo.cfg文件的路径）

​	服务器启动的入口，按照用户的配置启动QuorumPeer

2：`org.apache.zookeeper.server.quorum.QuorumPeerConfig#parseDynamicConfig`方法中会创建一个QuorumVerifier（用于检查一个服务器列表能否构成一个可用的服务器集群）

```java
	long getWeight(long id);
    boolean containsQuorum(Set<Long> set);//投票是否过半就是这里判断的
    long getVersion();
    void setVersion(long ver);
    Map<Long, QuorumServer> getAllMembers();//获取集群中所有的节点。
    Map<Long, QuorumServer> getVotingMembers();//获取集群中参与选举的节点。
    Map<Long, QuorumServer> getObservingMembers();//获取集群中Observer节点
    boolean equals(Object o);

如果zoo.cfg中没有配置分组或者权重，默认实例化的是
`org.apache.zookeeper.server.quorum.flexible.QuorumMaj#QuorumMaj(java.util.Properties)`
并会初始化一些数据，比如总共节点信息等，看下下面源码就好了，其实还是比较容易看懂的。
 public QuorumMaj(Properties props) throws ConfigException {
        for (Entry<Object, Object> entry : props.entrySet()) {
            String key = entry.getKey().toString();
            String value = entry.getValue().toString();

            if (key.startsWith("server.")) {
                int dot = key.indexOf('.');
                long sid = Long.parseLong(key.substring(dot + 1));
                QuorumServer qs = new QuorumServer(sid, value);
                allMembers.put(Long.valueOf(sid), qs);
                if (qs.type == LearnerType.PARTICIPANT)
                    votingMembers.put(Long.valueOf(sid), qs);
                else {
                    observingMembers.put(Long.valueOf(sid), qs);
                }
            } else if (key.equals("version")) {
                version = Long.parseLong(value, 16);
            }
        }
        half = votingMembers.size() / 2;
    }

public QuorumMaj(Map<Long, QuorumServer> allMembers) {
        this.allMembers = allMembers;
        for (QuorumServer qs : allMembers.values()) {
            if (qs.type == LearnerType.PARTICIPANT) {
                votingMembers.put(Long.valueOf(qs.id), qs);
            } else {
                observingMembers.put(Long.valueOf(qs.id), qs);
            }
        }
        half = votingMembers.size() / 2;
    }
```



3：``QuorumPeerConfig`` 看下名字应该就知道，主要是节点(法定人)有关配置,但是这里要注意一点，就是要在dataDir路径下，要有一个myid，不然就没有serverId，就拿不到QuorumServer

serverId,clientPortAddress，dataDir，dataLogDir等

4：`org.apache.zookeeper.server.quorum.QuorumPeerMain#runFromConfig` 要集群大于一才会进入这个方法的，否则就是单机了，单机ZooKeeperServerMain是这个启动类。

* ManagedUtil.registerLog4jMBeans() 注册日志需要的bean

* 创建节点直接的通信：ServerCnxnFactory(管理客户端的连接)，默认使用的是NIOServerCnxnFactory。当然也可以自己配置，使用Netty,觉得有必要对NIOServerCnxnFactory进行分析

  其实我个人觉得和Netty还是有些现实，毕竟都是基于NIO,首先在Netty中，我们一般也是两个线程组，一个是boss线程组，由于客户端的连接，一个是work线程组用于IO读写。ZK中的NIOServerCnxnFactory其实类似。首先看下`Set<SelectorThread> selectorThreads`,这是一个ZooKeeperThread线程组，就是用来进行读写的，默认数量是CPU核数的两倍(numWorkerThreads），另一个是`AcceptThread acceptThread`线程组，我觉得就是用来监听客户端的连接。

  ~~~java
  		numWorkerThreads = Integer.getInteger(
              ZOOKEEPER_NIO_NUM_WORKER_THREADS, 2 * numCores);
          workerShutdownTimeoutMS = Long.getLong(
              ZOOKEEPER_NIO_SHUTDOWN_TIMEOUT, 5000);
          for(int i=0; i<numSelectorThreads; ++i) {
              selectorThreads.add(new SelectorThread(i));
          }
  		//ServerSocketChannle
          this.ss = ServerSocketChannel.open();
          ss.socket().setReuseAddress(true);
          LOG.info("binding to port " + addr);
          ss.socket().bind(addr);
          ss.configureBlocking(false);
          acceptThread = new AcceptThread(ss, addr, selectorThreads);
  ~~~

  

* 初始化QuorumPeer,这也是一个ZK线程，看下initialize方法

  ```java
  public void initialize() throws SaslException {
          // init quorum auth server & learner
          if (isQuorumSaslAuthEnabled()) {
              Set<String> authzHosts = new HashSet<String>();
              for (QuorumServer qs : getView().values()) {
                  authzHosts.add(qs.hostname);
              }
              authServer = new SaslQuorumAuthServer(isQuorumServerSaslAuthRequired(),
                      quorumServerLoginContext, authzHosts);
              authLearner = new SaslQuorumAuthLearner(isQuorumLearnerSaslAuthRequired(),
                      quorumServicePrincipal, quorumLearnerLoginContext);
          } else {
              //这两个东西目前还不知道是干嘛的
              authServer = new NullQuorumAuthServer();
              authLearner = new NullQuorumAuthLearner();
          }
  ```

  

* 初始化完后，会对quorumPeer做很多赋值，这个东西应该是每个节点的核心了。前面做的一些初始化，比如QuorumVerifier和NIOServerCnxnFactory也都会放到quorumPeer中。

  `org.apache.zookeeper.server.quorum.QuorumPeerMain#runFromConfig`

  ```java
  		quorumPeer.setTxnFactory(new FileTxnSnapLog(
                        config.getDataLogDir(),
                        config.getDataDir()));//初始化快照和事务的文件
            quorumPeer.enableLocalSessions(config.areLocalSessionsEnabled());
            quorumPeer.enableLocalSessionsUpgrading(
                config.isLocalSessionsUpgradingEnabled());
            //quorumPeer.setQuorumPeers(config.getAllMembers());
            quorumPeer.setElectionType(config.getElectionAlg());
            quorumPeer.setMyid(config.getServerId());	//设置myid
            quorumPeer.setTickTime(config.getTickTime());
            quorumPeer.setMinSessionTimeout(config.getMinSessionTimeout());
            quorumPeer.setMaxSessionTimeout(config.getMaxSessionTimeout());
            quorumPeer.setInitLimit(config.getInitLimit());
            quorumPeer.setSyncLimit(config.getSyncLimit());
            quorumPeer.setConfigFileName(config.getConfigFilename());
  		  //每个节点有自己的zkDatabase,原来是放在这个quorumPeer中
            quorumPeer.setZKDatabase(new ZKDatabase(quorumPeer.getTxnFactory()));
            quorumPeer.setQuorumVerifier(config.getQuorumVerifier(), false);
            if (config.getLastSeenQuorumVerifier()!=null) {
                quorumPeer.setLastSeenQuorumVerifier(config.getLastSeenQuorumVerifier(), false);
            }
            quorumPeer.initConfigInZKDatabase();
            quorumPeer.setCnxnFactory(cnxnFactory);
            quorumPeer.setSecureCnxnFactory(secureCnxnFactory);
            quorumPeer.setSslQuorum(config.isSslQuorum());
            quorumPeer.setUsePortUnification(config.shouldUsePortUnification());
            quorumPeer.setLearnerType(config.getPeerType());
            quorumPeer.setSyncEnabled(config.getSyncEnabled());
            quorumPeer.setQuorumListenOnAllIPs(config.getQuorumListenOnAllIPs());
            if (config.sslQuorumReloadCertFiles) {
                quorumPeer.getX509Util().enableCertFileReloading();
            }
  
            // sets quorum sasl authentication configurations
            quorumPeer.setQuorumSaslEnabled(config.quorumEnableSasl);
            if(quorumPeer.isQuorumSaslAuthEnabled()){
                quorumPeer.setQuorumServerSaslRequired(config.quorumServerRequireSasl);
                quorumPeer.setQuorumLearnerSaslRequired(config.quorumLearnerRequireSasl);
                quorumPeer.setQuorumServicePrincipal(config.quorumServicePrincipal);
                quorumPeer.setQuorumServerLoginContext(config.quorumServerLoginContext);
                quorumPeer.setQuorumLearnerLoginContext(config.quorumLearnerLoginContext);
            }
            quorumPeer.setQuorumCnxnThreadsSize(config.quorumCnxnThreadsSize);
            quorumPeer.initialize();
            //这里肯定是要看一下的了
            quorumPeer.start();
            quorumPeer.join();//主线程阻塞到这
  ```

  接下来看下org.apache.zookeeper.server.quorum.QuorumPeer#start这个方法，简单理解就是启动这个线程了，但启动之前，还有去走其他的初始化，比如数据库....

  ```java
    @Override
      public synchronized void start() {
          if (!getView().containsKey(myid)) {
              throw new RuntimeException("My id " + myid + " not in the peer list");
           }
          loadDataBase();//初始化数据库(1)
          startServerCnxnFactory();//NIOServer线程启动（2） 待下面分析
          try {
              adminServer.start();
          } catch (AdminServerException e) {
              LOG.warn("Problem starting AdminServer", e);
              System.out.println(e);
          }
          startLeaderElection();
          super.start();//这里就是启动这个线程了
      }
  ```



5:`org.apache.zookeeper.server.quorum.QuorumPeer#loadDataBase`，对于zookeeper来说每个服务节点，都会有一个数据库，ZKDatabase 数据库，详细的数据保存在DataTree类中。

```java
             zkDb.loadDataBase();//数据加载
            // load the epochs
            //服务器在运行时产生的数据ID， zxid的值越大，表示数据越新
            long lastProcessedZxid = zkDb.getDataTree().lastProcessedZxid;
            long epochOfZxid = ZxidUtils.getEpochFromZxid(lastProcessedZxid);
            try {
                //当前选举轮数
                currentEpoch = readLongFromFile(CURRENT_EPOCH_FILENAME);
            } catch(FileNotFoundException e) {
            	currentEpoch = epochOfZxid; //如果没找到这个文件，就一当前epochOfZxid
            	LOG.info(CURRENT_EPOCH_FILENAME
            	        + " not found! Creating with a reasonable default of {}. This should only happen when you are upgrading your installation",
            	        currentEpoch);
            	writeLongToFile(CURRENT_EPOCH_FILENAME, currentEpoch);
            }
            if (epochOfZxid > currentEpoch) {
                //说明数据不是最新的
                throw new IOException("The current epoch, " + ZxidUtils.zxidToString(currentEpoch) + ", is older than the last zxid, " + lastProcessedZxid);
            }
            try {
                acceptedEpoch = readLongFromFile(ACCEPTED_EPOCH_FILENAME);
            } catch(FileNotFoundException e) {
            
            	writeLongToFile(ACCEPTED_EPOCH_FILENAME, acceptedEpoch);
            }
            if (acceptedEpoch < currentEpoch) {
                throw new IOException("The accepted epoch, " + ZxidUtils.zxidToString(acceptedEpoch) + " is less than the current epoch, " + ZxidUtils.zxidToString(currentEpoch));
            }
```



6:`org.apache.zookeeper.server.ZKDatabase#loadDataBase`

```java
 public long loadDataBase() throws IOException {
        long zxid = snapLog.restore(dataTree, sessionsWithTimeouts, commitProposalPlaybackListener);
        initialized = true;
        return zxid;
    }
    
  //主要还是要看下snapLog.restore方法，很明显，这是一个快照重新加载的方法，我记得zk的数据是会定时更新快照		//dataDir 这个文件会在加载zoo.cfg的时候初始化好。
  //那么说明快照中的数据就是新的，只要启动的时候，重新加载就可以。暂时这样理解吧。

 public long restore(DataTree dt, Map<Long, Integer> sessions,
                        PlayBackListener listener) throws IOException {
     	//看这个方法就好了
        long deserializeResult = snapLog.deserialize(dt, sessions);
        FileTxnLog txnLog = new FileTxnLog(dataDir);
        if (-1L == deserializeResult) {
 
            if (txnLog.getLastLoggedZxid() != -1) {
            
            save(dt, (ConcurrentHashMap<Long, Integer>)sessions);
            /* return a zxid of zero, since we the database is empty */
            return 0;
        }
        return fastForwardFromEdits(dt, sessions, listener);
           
  public long deserialize(DataTree dt, Map<Long, Integer> sessions)
            throws IOException {
        //这里已经按照版本进行排序了，最多100个哦
        List<File> snapList = findNValidSnapshots(100);
        if (snapList.size() == 0) {
            return -1L;
        }
        File snap = null;
        boolean foundValid = false;
        for (int i = 0, snapListSize = snapList.size(); i < snapListSize; i++) {
            snap = snapList.get(i);
            LOG.info("Reading snapshot " + snap);
            try (InputStream snapIS = new BufferedInputStream(new FileInputStream(snap));
                 CheckedInputStream crcIn = new CheckedInputStream(snapIS, new Adler32())) {
                InputArchive ia = BinaryInputArchive.getArchive(crcIn);
                //这个方法就不再进行展开看了，路面就是把节点zkNode加载出来呀，放到DataTree中去。
                deserialize(dt, sessions, ia);
                long checkSum = crcIn.getChecksum().getValue();
                long val = ia.readLong("val");
                if (val != checkSum) {
                    throw new IOException("CRC corruption in snapshot :  " + snap);
                }
                foundValid = true;
                break;
            } catch (IOException e) {
                LOG.warn("problem reading snap file " + snap, e);
            }
        }
        if (!foundValid) {
            throw new IOException("Not able to find valid snapshots in " + snapDir);
        }
        //最后这个snap是最新的
        dt.lastProcessedZxid = Util.getZxidFromName(snap.getName(), 		                          SNAPSHOT_FILE_PREFIX);
        return dt.lastProcessedZxid;
    }
```

上面这个过程就是zk服务重启后，是如何保证数据和先前一致性。

7：`org.apache.zookeeper.server.NIOServerCnxnFactory`在org.apache.zookeeper.server.quorum.QuorumPeer#startServerCnxnFactory中有调用这个方法。最后会调用到这里。

```java
 @Override
    public void start() {
        stopped = false;
        if (workerPool == null) {
            //工作线程池
            workerPool = new WorkerService(
                "NIOWorker", numWorkerThreads, false);
        }
        //工作线程启动
        for(SelectorThread thread : selectorThreads) {
            if (thread.getState() == Thread.State.NEW) {
                thread.start();
            }
        }
        // ensure thread is started once and only once
        //AcceptThread启动
        if (acceptThread.getState() == Thread.State.NEW) {
            acceptThread.start();
        }
        if (expirerThread.getState() == Thread.State.NEW) {
            expirerThread.start();
        }
```

* AcceptThread 

  ~~~java
  public void run() {
            try {
                while (!stopped && !acceptSocket.socket().isClosed()) {
                    try {
                          //这里看这个方法就好了
                         select(); //（1）
                     } catch (RuntimeException e) {
                          LOG.warn("Ignoring unexpected runtime exception", e);
                     } catch (Exception e) {
                          LOG.warn("Ignoring unexpected exception", e);
                     }
                  }
              } finally {
                 
              }
      
      private void select() {
              try {
                  selector.select();
                  Iterator<SelectionKey> selectedKeys =
                      selector.selectedKeys().iterator();
                  while (!stopped && selectedKeys.hasNext()) {//有连接
                      SelectionKey key = selectedKeys.next();
                      selectedKeys.remove();
  
                      if (!key.isValid()) {
                          continue;
                      }
                      if (key.isAcceptable()) {
                          if (!doAccept()) {//(2) 看这个方法就好了
                            
                              pauseAccept(10);
                          }
                      } else {
                          LOG.warn("Unexpected ops in accept select "
                                   + key.readyOps());
                      }
                  }
              } catch (IOException e) {
                  LOG.warn("Ignoring IOException while selecting", e);
              }
          }
      
       private boolean doAccept() {
              try {
                  sc = acceptSocket.accept();
                  accepted = true;
                  InetAddress ia = sc.socket().getInetAddress();//socket
                  int cnxncount = getClientCnxnCount(ia);
  
           
                  LOG.debug("Accepted socket connection from "
                           + sc.socket().getRemoteSocketAddress());
                  sc.configureBlocking(false);
  
                  // Round-robin assign this connection to a selector thread
                  if (!selectorIterator.hasNext()) {
                      selectorIterator = selectorThreads.iterator();
                  }
                  //还记得开始吗 我们说的这个selector线程组默认是两个，但这里怎么看，是不是发现也会
                  //只用到第一个线程 怎么感觉和Netty的bossGroup是一样的
                  SelectorThread selectorThread = selectorIterator.next();
                  //这里就不进去看了
                  if (!selectorThread.addAcceptedConnection(sc)) {
                      throw new IOException(
                          "Unable to add connection to selector queue"
                          + (stopped ? " (shutdown in progress)" : ""));
                  }
                  acceptErrorLogger.flush();
              } catch (IOException e) {
                  // accept, maxClientCnxns, configureBlocking
                  acceptErrorLogger.rateLimitLog(
                      "Error accepting new connection: " + e.getMessage());
                  fastCloseSock(sc);
              }
              return accepted;
          }
  ~~~

  

* `selectorThreads`工作线程组，里面是`SelectorThread`，这里看下这个线程

  ```java
  SelectorThread
       public SelectorThread(int id) throws IOException {
              super("NIOServerCxnFactory.SelectorThread-" + id);
              this.id = id;
      		//这个队列我觉得只有AcceptThread线程才会用到 只有处理连接的时候才会用到
              acceptedQueue = new LinkedBlockingQueue<SocketChannel>();
      		//
              updateQueue = new LinkedBlockingQueue<SelectionKey>();
          }
  		
  		//这个方法会把IO操作放到这个阻塞队列尾部，按理这里对于AcceptThread线程组来说，也是没用到的
  		 public boolean addInterestOpsUpdateRequest(SelectionKey sk) {
              if (stopped || !updateQueue.offer(sk)) {
                  return false;
              }
              wakeupSelector();
              return true;
          }
  
  	
  	
  
  ```

  **这里分析是针对zk自己与其他客户端的通信，就是默认端口2181，当然这个可以自己配置，但一般使用默认的，不知道上面理解对不对**

  `org.apache.zookeeper.server.NIOServerCnxnFactory.SelectorThread#handleIO`再看下这个方法

  ```java
   private void handleIO(SelectionKey key) {
              IOWorkRequest workRequest = new IOWorkRequest(this, key);
              NIOServerCnxn cnxn = (NIOServerCnxn) key.attachment();
  
              // Stop selecting this key while processing on its
              // connection
              cnxn.disableSelectable();
              key.interestOps(0);
              touchCnxn(cnxn);
              workerPool.schedule(workRequest);
          }
  	//看到这里应该理解很多了，就是会把请求封装成一个workRequest,然后放到workerPool线程池中去，这里就相当于一个业务线程。所以具体逻辑应该是在workRequest中doWork方法
          public void doWork() throws InterruptedException {
              if (!key.isValid()) {
                  selectorThread.cleanupSelectionKey(key);
                  return;
              }
  
              if (key.isReadable() || key.isWritable()) {
                  //接下来应该是要看这个方法哦
                  cnxn.doIO(key);
                  // Check if we shutdown or doIO() closed this connection
                  if (stopped) {
                      cnxn.close();
                      return;
                  }
                  if (!key.isValid()) {
                      selectorThread.cleanupSelectionKey(key);
                      return;
                  }
                  touchCnxn(cnxn);
              }
              // Mark this connection as once again ready for selection
              cnxn.enableSelectable();
              // Push an update request on the queue to resume selecting
              // on the current set of interest ops, which may have changed
              // as a result of the I/O operations we just performed.
              if (!selectorThread.addInterestOpsUpdateRequest(key)) {
                  cnxn.close();
              }
          }
  ```

  

  8：接下来要看的是NIOServerCnxn里面的方法`org.apache.zookeeper.server.NIOServerCnxn#doIO`

  从上面可以看到，doWork中会调用这个方法。

  ```java
  void doIO(SelectionKey k) throws InterruptedException {
          try {
           
              if (k.isReadable()) {//这里是读Io
                  int rc = sock.read(incomingBuffer);
                
                  if (incomingBuffer.remaining() == 0) {
                      if (isPayload) { // not the case for 4letterword
                          readPayload();//看这个方法（1）
                      }
                      else {
                   
                          return;
                      }
                  }
              }
              if (k.isWritable()) {//写IO
                  handleWrite(k);//(2)
              }
          } 
      }
      private void readPayload() throws IOException, InterruptedException {
          if (incomingBuffer.remaining() != 0) { // have we read length bytes?
              int rc = sock.read(incomingBuffer); // sock is non-blocking, so ok
              if (rc < 0) {
              }
          }
  
          if (incomingBuffer.remaining() == 0) { // have we read length bytes?
              packetReceived();
              incomingBuffer.flip();
              if (!initialized) {//没有初始化 那么就是连接请求
                  readConnectRequest();
              } else {
                  //真正的请求逻辑实现
                  readRequest();
              }
              lenBuffer.clear();
              incomingBuffer = lenBuffer;
          }
      }
  ```

  这里就分析到这里把，感觉自己有点绕进去了，总之上面写的都是节点启动的初始化，和节点直接的连接和通信。但还有问题没解决。比如ZookeeperServer是如何初始化的？但Server还没初始化呀，这个肯定是要等leader选举完成后才能确定的。

9：接下来要回到开始的时候了，还记得QuorumPeer的start方法吗，这里再把代码贴一下吧

```java
 @Override
    public synchronized void start() {
        if (!getView().containsKey(myid)) {
            throw new RuntimeException("My id " + myid + " not in the peer list");
         }
        loadDataBase();
        startServerCnxnFactory();
        try {
            adminServer.start();//这里暂时不管
        } catch (AdminServerException e) {
          
        }
        //到这里上面的其实已经分析的差不多了，接下来就是看这个方法
        startLeaderElection();
        super.start();
    }
```



10:节点间的通信过程`QuorumCnxManager`:管理器

```java
 private void starter(QuorumPeer self, QuorumCnxManager manager) {
        this.self = self;
        proposedLeader = -1;
        proposedZxid = -1;
		//看名字就知道这两个阻塞队列的意思，一个是用来发送信息的，一个是接收通知的
        sendqueue = new LinkedBlockingQueue<ToSend>();
        recvqueue = new LinkedBlockingQueue<Notification>();
     	//这个messenger是比较关键的
        this.messenger = new Messenger(manager);
    }
	
	//Messenger 构造方法
	 Messenger(QuorumCnxManager manager) {
			//sender线程，用来发送sendqueue这个队列里面的任务
            this.ws = new WorkerSender(manager);
		
            this.wsThread = new Thread(this.ws,
                    "WorkerSender[myid=" + self.getId() + "]");
            this.wsThread.setDaemon(true);
			
         	//接收线程
            this.wr = new WorkerReceiver(manager);

            this.wrThread = new Thread(this.wr,
                    "WorkerReceiver[myid=" + self.getId() + "]");
            this.wrThread.setDaemon(true);
        }
		//最后启动上面这两个线程
 		void start(){
            this.wsThread.start();
            this.wrThread.start();
        }

```

 * WorkerSender/WorkerReceiver   SendWorker/RecvWorker 把这两组线程搞懂，就能明白节点直接消息的通信

   服务节点会把要notify的信息，封装撑一个ToSend对象,放到workerSender的队列中，然后在process()方法中构建要放得信息ByteBuffer,，接着是`org.apache.zookeeper.server.quorum.QuorumCnxManager#toSend`调用这个方法，把消息加到sid对应的ConcurrentHashMap<Long, ArrayBlockingQueue<ByteBuffer>> queueSendMap中。如果是第一次，那么就需要建立socket连接，具体可以看下connectOne(sid)这个方法。

   ```java
   //里面很多代码已经被我删除了，就留下核心代码
   synchronized private boolean connectOne(long sid, InetSocketAddress electionAddr){
           if (senderWorkerMap.get(sid) != null) {
               //说明这个连接已经存在了
               LOG.debug("There is a connection already for server " + sid);
               return true;
           }
           Socket sock = null;
           try {
               	//简历socket连接
                    sock = new Socket();
                    setSockOpts(sock);
                    sock.connect(electionAddr, cnxTO);
               
               if (quorumSaslAuthEnabled) {
                   //异步连接
                   //我猜这里面肯定是要把这份socket连接缓存，并且肯定是有一天线程来专门处理
                   initiateConnectionAsync(sock, sid);
               } else {
                   //同步连接
                   initiateConnection(sock, sid);
               }
               return true;
       }
       
       //就分析上面异步简历连接的这个方法，和同步最终结果应该是一样的，异步最后也会调用同步的这方法，就是简单的专门放到了另一个线程池去处理
        public void initiateConnectionAsync(final Socket sock, final Long sid) {
           if(!inprogressConnections.add(sid)){
               closeSocket(sock);
               return;
           }
           try {
               //很明显，这是专门一个线程池来处理的。所以接下来看QuorumConnectionReqThread
               connectionExecutor.execute(
                       new QuorumConnectionReqThread(sock, sid));
               connectionThreadCnt.incrementAndGet();
           }
       }
   
       //下面这个方法是最终建立连接后，可以完成通信了
        private boolean startConnection(Socket sock, Long sid)
               throws IOException {
           DataOutputStream dout = null;
           DataInputStream din = null;
           try {
           
               dout = new DataOutputStream(buf);
   
               dout.writeLong(PROTOCOL_VERSION);
               dout.writeLong(self.getId());
               final InetSocketAddress electionAddr = self.getElectionAddress();
               String addr = electionAddr.getHostString() + ":" + electionAddr.getPort();
               byte[] addr_bytes = addr.getBytes();
               dout.writeInt(addr_bytes.length);
               dout.write(addr_bytes);
               dout.flush();
   
               din = new DataInputStream(
                       new BufferedInputStream(sock.getInputStream()));
           } catch (IOException e) {
               return false;
           }
   
           QuorumPeer.QuorumServer qps = self.getVotingView().get(sid);
           if (qps != null) 
               authLearner.authenticate(sock, qps.hostname);
           }
   
           // If lost the challenge, then drop the new connection
           if (sid > self.getId()) {//TODO 这里暂时不懂什么意思
               LOG.info("Have smaller server identifier, so dropping the " +
                       "connection: (" + sid + ", " + self.getId() + ")");
               closeSocket(sock);
               // Otherwise proceed with the connection
           } else {
              	//这里看到sendWorker了吧，持有和一个节点的通信socket，最终的消息有这里发送
               SendWorker sw = new SendWorker(sock, sid);
               //同样RecvWorker,看到这里应该能想到，这个线程会把读任务放到
               //org.apache.zookeeper.server.quorum.QuorumCnxManager#recvQueue中
               //然后由WorkerReceiver线程来进行消耗
               RecvWorker rw = new RecvWorker(sock, din, sid, sw);
               sw.setRecv(rw);
   			
               SendWorker vsw = senderWorkerMap.get(sid);
   
               if(vsw != null)
                   vsw.finish();
   			//缓存到org.apache.zookeeper.server.quorum.QuorumCnxManager#senderWorkerMap
               senderWorkerMap.put(sid, sw);
               
               queueSendMap.putIfAbsent(sid, new ArrayBlockingQueue<ByteBuffer>(
                       SEND_CAPACITY));
               sw.start();
               rw.start();
   
               return true;
   
           }
           return false;
       }
       
   ```

   **注意这里的通信，应该是zk集群Leader选举之间的通信**



**问题：节点之间心跳是如何实现的**?

我觉得会是在sendWork和ReceiveWork中，但是没找到，目前我觉得就是基于节点间的连接通信呀。

看下Leader Follower Observer中的几个type类型就明白了。心跳 commint ack都是走的这一块通信

```java
//leader中的ping 但这里不是上面我猜想的选举通信的Socket
for (LearnerHandler f : getLearners()) {
    f.ping();
}
```



### Leader选举过程

节点启动时，节点的初始状态` ServerState.LOOKING`状态，此时ZK会获取当前的Leader或者会发起一个选举。

zookeeper提供了三种选举算法，默认的是`FastLeaderElection`，我们就看这个源码

先连接下几个选举的相关类：

```java
QuorumVerifier用于检查一个服务器列表能否构成一个可用的服务器集群

SyncedLearnerTracker用于保存集群和选举的映射关系

QuorumCnxManager负责选举中的连接管理，
  
```

在启动后，其实就是QuorumPeer.start()方法后，zk会调用

`org.apache.zookeeper.server.quorum.QuorumPeer#startLeaderElection` 选举的入口

```java
 synchronized public void startLeaderElection() {
       try {
           //开始的状态是LOOKING
           if (getPeerState() == ServerState.LOOKING) {
               currentVote = new Vote(myid, getLastLoggedZxid(), getCurrentEpoch());
           }
       } catch(IOException e) {
          
           throw re;
       }
        if (electionType == 0) {
            try {
                udpSocket = new DatagramSocket(getQuorumAddress().getPort());
                responder = new ResponderThread();
                //这个地方我个人觉得就是一个心跳，把自己的节点状态广播出去
                responder.start();
            } catch (SocketException e) {
                throw new RuntimeException(e);
            }
        }
        this.electionAlg = createElectionAlgorithm(electionType);
    }

```

`org.apache.zookeeper.server.quorum.FastLeaderElection#lookForLeader`,这个方法里面关注下`sendNotifications()`这个方法。

```java
 private void sendNotifications() {
        for (long sid : self.getCurrentAndNextConfigVoters()) {
            QuorumVerifier qv = self.getQuorumVerifier();
            ToSend notmsg = new ToSend(ToSend.mType.notification,
                    proposedLeader,
                    proposedZxid,
                    logicalclock.get(),
                    QuorumPeer.ServerState.LOOKING,
                    sid,
            }
            sendqueue.offer(notmsg);
        }
     //这里就是把自己当前状态广播出去。
     //数据发送过程其实在上面已经分析过了
   
```

在上面其实我们已经分析了节点通信的流程，然后这里再仔细说一下具体的实现过程吧。

> 当FastLeaderElection要发送数据时，会通过向sendqueue发送数据来异步调用WorkerSender.process。在QuorumCnxManager.toSend的实现中，若到发送目标节点的连接不存在，则会主动建立连接。QuorumCnxManager通过加数据添加到发送列表来异步调用SendWorker.send。
>
> 在接收端收到新连接时，首先会解析发送方发送的头信息。为了保证两个节点之间只有一个连接（即不会出现A建立连接到B，同时B也建立连接到A），zk会检查建立连接的节点的id。若发起连接的节点id小于当前节点，zk会断开这连接，并且主动建立一个到对方节点的连接。
>
> RecvWorker在收到数据库，调用addToRecvQueue将数据添加到队列，WorkerReceiver从队列取数据处理。WorkerReceiver处理逻辑如下:
>
> - 检查对方节点是否参与选举，不参与选举的话直接返回当前的节点的结果。
> - 若当前节点也在LOOKING状态，则将对方节点选举的结果添加到接收队列。若对方节点处于LOOKING状态，且状态落后于当前节点，则发送当前节点的选举信息到对方节点。
> - 若当前节点处于选举成功状态(不是LOOKING状态)，则发送当前节点的选举结果到对方节点。

**zxid**

选举中会使用到数据库中最新的事务id(ZKDatabase.getDataTreeLastProcessedZxid()), 事务id由两部分组成，高32位表示Epoch，每次有新的节点当选会+1(Leader.lead())；后32位为事务id，有新的节点当选，该部分会从0重新开始。当数据库有写操作时，整个事务id会+1（参考ZooKeeperServer.getNextZxid()的引用）

**ElectionEpoch**

当前节点的ElectionEpoch定义于FastLeaderElection.logicalclock中，每发起一次新选举，该值会自+1。若其他节点选举信息中的ElectionEpoch大于当前节点，则会设置当前节点的值为接收到的值。

对这么上面文字描述中对应代码实现，我贴出来看下。

```java
			org.apache.zookeeper.server.quorum.FastLeaderElection.Messenger.WorkerReceiver#run
		//上面代码我省略了，                
		if(self.getPeerState() == QuorumPeer.ServerState.LOOKING){//判断自己状态是否为LOOKING
           	//将notify 放入接收队列中 这个队列中的数据，会在lookForLeader方法中消耗
            recvqueue.offer(n);
            	//对方是LOOKING状态 并且对方electionEpoch < 自己（选举落后于自己）
               if((ackstate == QuorumPeer.ServerState.LOOKING)
                                        && (n.electionEpoch < logicalclock.get())){
                    Vote v = getVote();
                    QuorumVerifier qv = self.getQuorumVerifier();
                    ToSend notmsg = new ToSend(ToSend.mType.notification,
                                            v.getId(),
                                            v.getZxid(),
                                            logicalclock.get(),
                                            self.getPeerState(),
                                            response.sid,
                                            v.getPeerEpoch(),
                                            qv.toString().getBytes());
                   	//发送自己节点状态信息
                     sendqueue.offer(notmsg);
                    }
              } else {
            		//自己选举已经确定，不处理，只广播自己状态
                      Vote current = self.getCurrentVote();
                      f(ackstate == QuorumPeer.ServerState.LOOKING){
                      		 QuorumVerifier qv = self.getQuorumVerifier();
                             ToSend notmsg = new ToSend(
                                            ToSend.mType.notification,
                                            current.getId(),
                                            current.getZxid(),
                                            current.getElectionEpoch(),
                                            self.getPeerState(),
                                            response.sid,
                                            current.getPeerEpoch(),
                                            qv.toString().getBytes());
                                    sendqueue.offer(notmsg);
                                }
                            }
                        }

//接着肯定是看lookForLeader()方法了，很长。直接看while()循环体中
  while ((self.getPeerState() == ServerState.LOOKING) &&
                    (!stop)){
              	//接收上面传过来的notify
                Notification n = recvqueue.poll(notTimeout,
                        TimeUnit.MILLISECONDS);
                if(n == null){//没有接收到任何有效通知
                    if(manager.haveDelivered()){//已经连接过的，直接广播
                        sendNotifications();
                    } else {
                        manager.connectAll();//先建立连接
                    }

                } else if (validVoter(n.sid) && validVoter(n.leader)) {
                    switch (n.state) {
                    case LOOKING:
                        // If notification > current, replace and send messages out
                        if (n.electionEpoch > logicalclock.get()) {
                            //直接设置最新的选举轮数
                            logicalclock.set(n.electionEpoch);
                            recvset.clear();//情况收到的投票信息
                            //这里和权重有关
                            if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch,
                                    getInitId(), getInitLastLoggedZxid(), getPeerEpoch())) {			//更新当前节点预选的leaderId he Zxid信息
                                updateProposal(n.leader, n.zxid, n.peerEpoch);
                            } else {
                                //更新当前节点预选的leaderId he Zxid信息
                                updateProposal(getInitId(),
                                        getInitLastLoggedZxid(),
                                        getPeerEpoch());
                            }
                            //广播节点信息
                            sendNotifications();
                        } else if (n.electionEpoch < logicalclock.get()) {
                            //再次确定是否比自己选举选举轮数要小
                            break;
                        } else if (totalOrderPredicate(n.leader, n.zxid, n.peerEpoch,
                                proposedLeader, proposedZxid, proposedEpoch)) {
                            updateProposal(n.leader, n.zxid, n.peerEpoch);
                            sendNotifications();
                        }
                        // don't care about the version if it's in LOOKING state
                        //记录投票信息
                        recvset.put(n.sid, new Vote(n.leader, n.zxid, n.electionEpoch, n.peerEpoch));
                         //这个if里面就是根据投票信息，判断确定是否过半，确定节点状态
                        if (termPredicate(recvset,
                                new Vote(proposedLeader, proposedZxid,
                                        logicalclock.get(), proposedEpoch))) {

                            // Verify if there is any change in the proposed leader
                            //没有再修改
                            while((n = recvqueue.poll(finalizeWait,
                                    TimeUnit.MILLISECONDS)) != null){
                                if(totalOrderPredicate(n.leader, n.zxid, n.peerEpoch,
                                        proposedLeader, proposedZxid, proposedEpoch)){
                                    recvqueue.put(n);
                                    break;
                                }
                            }
                            /*
                             * This predicate is true once we don't read any new
                             * relevant message from the reception queue
                             */
                            if (n == null) {
                                //如果上面updateProposal方法中是将自己设置为leader的话，这里会最终								确定，如果是的话自己就是为Leadr,否则为Follower或者Observer
                                self.setPeerState((proposedLeader == self.getId()) ?
                                        ServerState.LEADING: learningState());
                                Vote endVote = new Vote(proposedLeader,
                                        proposedZxid, logicalclock.get(), 
                                        proposedEpoch);
                                //清楚投票信息接收
                                leaveInstance(endVote);
                                return endVote;
                            }
                        }
                        break;
                    case OBSERVING://不参与选举投票
                        LOG.debug("Notification from observer: " + n.sid);
                        break;
                    case FOLLOWING:
                    case LEADING:
                    }
        }
    }

```

选举完成后，就会确定自己的状态，然后在QuorumPeer会创建节点zkServer和节点(Leader/follower)



### 客户端连接管理

其实在上面启动过程开始就有分析到，在zk中用户可以配置两种连接，第一种是使用Java NIO接口，对应的类为NIOServerCnxn以及NIOServerCnxnFactory。第二种使用Netty库，对应的类为NettyServerCnxn和NettyServerCnxnFactory。默认的好像是NIOServerCnxnFactory，这个就是基于JavaNIO实现的，其实我觉得和Netty实现原理是一样的，上面过程我也是看的NIOServerCnxnFactory源码。



不管是使用Netty还是JavaNIO，客户端的请求最终会调用ZooKeeperServer.processPacket，而processPacket内部会调用submitRequest，最后submitRequest会将Request派发到firstProcessor.



要注意的是Zookeeper不同的角色的节点通过创建不同的Processors链表来处理客户端请求

* Follower节点配置Processors详见FollowerZooKeeperServer.setupRequestProcessor
* Leader节点配置Processors详见LeaderZooKeeperServer.setupRequestProcessors
* 

#### 请求处理

Zookeeper将客户端的请求实际上分为两种类型。第一种类型的请求可以叫做读请求，如getData，getChildren等。这种类型的请求不会修改zookeeper数据库的内容，因为zookeeper每个节点的数据库内容是一致的(不同的节点之间由于同步的时间不一致，可能会导致数据有不一致)，所以读请求只用在本地节点的数据库即可。第二种节点会修改数据库内容，如create,delete等。非Leader节点接收到此种请求，会先转发到Leader节点，Leader会按照先Proposal，后Commit的方式将请求同步到所有节点。



#### 处理流程

//TODO

处理请求的核心就是如何将写操作同步到所有服务器。

##### Leader的处理流程

* LeaderRequestProcessor

  如果是LocalSession，并且用户请求为创建临时节点（Session关闭时节点会删除），则升级Session到GlobalSession，并且创建一条createSession请求（upgradeRequest）。

  如果createSession请求不为null，转发到PrepRequestProcessor。

  转发用户请求到PrepRequestProcessor

* PrepRequestProcessor

  PrepRequestProcessor设置Request的transcation信息，该Processor运行于独立的线程中。对于会改写zookeeper数据库的操作，该Processor会添加transcation信息，并且增加TxnHeader。对于读操作，则只检查session合法性。

  

  

*  ProposalRequestProcessor

  ProposalRequestProcessor会先将请求派发给CommitProcessor,随后ProposalRequestProcessor检查哪些请求被添加了transcation信息，如果被添加了transcation信息，则会发送Proposal包给集群内所有节点。发送完Proposal之后，再将该请求发送到SyncRequestProcessor

* SyncRequestProcessor 

  SyncRequestProcessor用于存储请求Transcation日志以及数据库的Snapshot日志到文件

  

* AckRequestProcessor 

  在SyncRequestProcessor存储日志之后，AckRequestProcessor仅仅是发送一个正在处理请求的ACK到Leader自己，用来模拟收到Leader节点的ACK

* Leader Proposal ACK管理

* 

* FinalRequestProcessor



	##### Follower的处理流程

CommitProcessor和FinalRequestProcessor前面已说明，所以此处只用介绍FollowerRequestProcessor。

* FollowerRequestProcessor

  FollowerRequestProcessor运行于独立的线程中。主要工作如下:

  ​		提交Request时，如果Request是local session，且创建了临时节点，则升级session到Global类型，并且		插入一个createSession request。

  将Request转发到CommitProcessor。

  如果为写请求，转发到Leader。

  

### 数据同步和证唯一性的保证

简单理解，就是在创建和修改的时候，非leader节点的话会转发到leader，此时leader等待是否事务投票是否过半，然后确定是否commit事务，如果是leader的话，也要收到过半的节点ACK后才能提交事务。

### TODO
    假如一个节点挂了后，重新选举过程？


以上内容部分文字描述参考[zookeeper源码分析](https://github.com/liwanghong/ZooKeeper-)




