layout:     post
title:      zookeeper
subtitle:   zookeeper源码分析
date:       2020-03-30
author:     tryingpfq
header-img: img/post-bg-zookeeper1.jpg
catalog: true
tags:

    - zookeeper


> 就最近不是很忙，在看分布式有关的东西，然后就有用到zookeeper，觉得好奇，就打算把源码看看。

看这篇之前，如果想先对zk有个感性认识，可以看下[上一篇](http://tryingpfq.club/2019/07/14/zookeeper/)

### 源码构建

这里就不多说了，参考：[zookeeper源码构建](https://segmentfault.com/a/1190000021451833)



### ZK集群模式启动过程

1：启动类：QuorumPeerMain（要设置main方法参数，就在zoo.cfg文件的路径）

​	服务器启动的入口，按照用户的配置启动QuorumPeer

2：`org.apache.zookeeper.server.quorum.QuorumPeerConfig#parseDynamicConfig`方法中会创建一个QuorumVerifier（用于检查一个服务器列表能否构成一个可用的服务器集群）

```java
	long getWeight(long id);
    boolean containsQuorum(Set<Long> set);//投票是否过半就是这里判断的
    long getVersion();
    void setVersion(long ver);
    Map<Long, QuorumServer> getAllMembers();//获取集群中所有的节点。
    Map<Long, QuorumServer> getVotingMembers();//获取集群中参与选举的节点。
    Map<Long, QuorumServer> getObservingMembers();//获取集群中Observer节点
    boolean equals(Object o);

如果zoo.cfg中没有配置分组或者权重，默认实例化的是
`org.apache.zookeeper.server.quorum.flexible.QuorumMaj#QuorumMaj(java.util.Properties)`
并会初始化一些数据，比如总共节点信息等，看下下面源码就好了，其实还是比较容易看懂的。
 public QuorumMaj(Properties props) throws ConfigException {
        for (Entry<Object, Object> entry : props.entrySet()) {
            String key = entry.getKey().toString();
            String value = entry.getValue().toString();

            if (key.startsWith("server.")) {
                int dot = key.indexOf('.');
                long sid = Long.parseLong(key.substring(dot + 1));
                QuorumServer qs = new QuorumServer(sid, value);
                allMembers.put(Long.valueOf(sid), qs);
                if (qs.type == LearnerType.PARTICIPANT)
                    votingMembers.put(Long.valueOf(sid), qs);
                else {
                    observingMembers.put(Long.valueOf(sid), qs);
                }
            } else if (key.equals("version")) {
                version = Long.parseLong(value, 16);
            }
        }
        half = votingMembers.size() / 2;
    }

public QuorumMaj(Map<Long, QuorumServer> allMembers) {
        this.allMembers = allMembers;
        for (QuorumServer qs : allMembers.values()) {
            if (qs.type == LearnerType.PARTICIPANT) {
                votingMembers.put(Long.valueOf(qs.id), qs);
            } else {
                observingMembers.put(Long.valueOf(qs.id), qs);
            }
        }
        half = votingMembers.size() / 2;
    }
```



3：``QuorumPeerConfig`` 看下名字应该就知道，主要是节点(法定人)有关配置,但是这里要注意一点，就是要在dataDir路径下，要有一个myid，不然就没有serverId，就拿不到QuorumServer

serverId,clientPortAddress，dataDir，dataLogDir等

4：`org.apache.zookeeper.server.quorum.QuorumPeerMain#runFromConfig` 要集群大于一才会进入这个方法的，否则就是单机了，单机ZooKeeperServerMain是这个启动类。

* ManagedUtil.registerLog4jMBeans() 注册日志需要的bean

* 创建节点直接的通信：ServerCnxnFactory(管理客户端的连接)，默认使用的是NIOServerCnxnFactory。当然也可以自己配置，使用Netty,觉得有必要对NIOServerCnxnFactory进行分析

  其实我个人觉得和Netty还是有些现实，毕竟都是基于NIO,首先在Netty中，我们一般也是两个线程组，一个是boss线程组，由于客户端的连接，一个是work线程组用于IO读写。ZK中的NIOServerCnxnFactory其实类似。首先看下`Set<SelectorThread> selectorThreads`,这是一个ZooKeeperThread线程组，就是用来进行读写的，默认数量是CPU核数的两倍(numWorkerThreads），另一个是`AcceptThread acceptThread`线程组，我觉得就是用来监听客户端的连接。

  ~~~java
  		numWorkerThreads = Integer.getInteger(
              ZOOKEEPER_NIO_NUM_WORKER_THREADS, 2 * numCores);
          workerShutdownTimeoutMS = Long.getLong(
              ZOOKEEPER_NIO_SHUTDOWN_TIMEOUT, 5000);
          for(int i=0; i<numSelectorThreads; ++i) {
              selectorThreads.add(new SelectorThread(i));
          }
  		//ServerSocketChannle
          this.ss = ServerSocketChannel.open();
          ss.socket().setReuseAddress(true);
          LOG.info("binding to port " + addr);
          ss.socket().bind(addr);
          ss.configureBlocking(false);
          acceptThread = new AcceptThread(ss, addr, selectorThreads);
  ~~~

  

* 初始化QuorumPeer,这也是一个ZK线程，看下initialize方法

  ```java
  public void initialize() throws SaslException {
          // init quorum auth server & learner
          if (isQuorumSaslAuthEnabled()) {
              Set<String> authzHosts = new HashSet<String>();
              for (QuorumServer qs : getView().values()) {
                  authzHosts.add(qs.hostname);
              }
              authServer = new SaslQuorumAuthServer(isQuorumServerSaslAuthRequired(),
                      quorumServerLoginContext, authzHosts);
              authLearner = new SaslQuorumAuthLearner(isQuorumLearnerSaslAuthRequired(),
                      quorumServicePrincipal, quorumLearnerLoginContext);
          } else {
              //这两个东西目前还不知道是干嘛的
              authServer = new NullQuorumAuthServer();
              authLearner = new NullQuorumAuthLearner();
          }
  ```

  

* 初始化完后，会对quorumPeer做很多赋值，这个东西应该是每个节点的核心了。前面做的一些初始化，比如QuorumVerifier和NIOServerCnxnFactory也都会放到quorumPeer中。

  `org.apache.zookeeper.server.quorum.QuorumPeerMain#runFromConfig`

  ```java
  		quorumPeer.setTxnFactory(new FileTxnSnapLog(
                        config.getDataLogDir(),
                        config.getDataDir()));//初始化快照和事务的文件
            quorumPeer.enableLocalSessions(config.areLocalSessionsEnabled());
            quorumPeer.enableLocalSessionsUpgrading(
                config.isLocalSessionsUpgradingEnabled());
            //quorumPeer.setQuorumPeers(config.getAllMembers());
            quorumPeer.setElectionType(config.getElectionAlg());
            quorumPeer.setMyid(config.getServerId());	//设置myid
            quorumPeer.setTickTime(config.getTickTime());
            quorumPeer.setMinSessionTimeout(config.getMinSessionTimeout());
            quorumPeer.setMaxSessionTimeout(config.getMaxSessionTimeout());
            quorumPeer.setInitLimit(config.getInitLimit());
            quorumPeer.setSyncLimit(config.getSyncLimit());
            quorumPeer.setConfigFileName(config.getConfigFilename());
  		  //每个节点有自己的zkDatabase,原来是放在这个quorumPeer中
            quorumPeer.setZKDatabase(new ZKDatabase(quorumPeer.getTxnFactory()));
            quorumPeer.setQuorumVerifier(config.getQuorumVerifier(), false);
            if (config.getLastSeenQuorumVerifier()!=null) {
                quorumPeer.setLastSeenQuorumVerifier(config.getLastSeenQuorumVerifier(), false);
            }
            quorumPeer.initConfigInZKDatabase();
            quorumPeer.setCnxnFactory(cnxnFactory);
            quorumPeer.setSecureCnxnFactory(secureCnxnFactory);
            quorumPeer.setSslQuorum(config.isSslQuorum());
            quorumPeer.setUsePortUnification(config.shouldUsePortUnification());
            quorumPeer.setLearnerType(config.getPeerType());
            quorumPeer.setSyncEnabled(config.getSyncEnabled());
            quorumPeer.setQuorumListenOnAllIPs(config.getQuorumListenOnAllIPs());
            if (config.sslQuorumReloadCertFiles) {
                quorumPeer.getX509Util().enableCertFileReloading();
            }
  
            // sets quorum sasl authentication configurations
            quorumPeer.setQuorumSaslEnabled(config.quorumEnableSasl);
            if(quorumPeer.isQuorumSaslAuthEnabled()){
                quorumPeer.setQuorumServerSaslRequired(config.quorumServerRequireSasl);
                quorumPeer.setQuorumLearnerSaslRequired(config.quorumLearnerRequireSasl);
                quorumPeer.setQuorumServicePrincipal(config.quorumServicePrincipal);
                quorumPeer.setQuorumServerLoginContext(config.quorumServerLoginContext);
                quorumPeer.setQuorumLearnerLoginContext(config.quorumLearnerLoginContext);
            }
            quorumPeer.setQuorumCnxnThreadsSize(config.quorumCnxnThreadsSize);
            quorumPeer.initialize();
            //这里肯定是要看一下的了
            quorumPeer.start();
            quorumPeer.join();//主线程阻塞到这
  ```

  接下来看下org.apache.zookeeper.server.quorum.QuorumPeer#start这个方法，简单理解就是启动这个线程了，但启动之前，还有去走其他的初始化，比如数据库....

  ```java
    @Override
      public synchronized void start() {
          if (!getView().containsKey(myid)) {
              throw new RuntimeException("My id " + myid + " not in the peer list");
           }
          loadDataBase();//初始化数据库(1)
          startServerCnxnFactory();//NIOServer线程启动（2） 待下面分析
          try {
              adminServer.start();
          } catch (AdminServerException e) {
              LOG.warn("Problem starting AdminServer", e);
              System.out.println(e);
          }
          startLeaderElection();
          super.start();//这里就是启动这个线程了
      }
  ```



5:`org.apache.zookeeper.server.quorum.QuorumPeer#loadDataBase`，对于zookeeper来说每个服务节点，都会有一个数据库，ZKDatabase 数据库，详细的数据保存在DataTree类中。

```java
             zkDb.loadDataBase();//数据加载
            // load the epochs
            //服务器在运行时产生的数据ID， zxid的值越大，表示数据越新
            long lastProcessedZxid = zkDb.getDataTree().lastProcessedZxid;
            long epochOfZxid = ZxidUtils.getEpochFromZxid(lastProcessedZxid);
            try {
                //当前选举轮数
                currentEpoch = readLongFromFile(CURRENT_EPOCH_FILENAME);
            } catch(FileNotFoundException e) {
            	currentEpoch = epochOfZxid; //如果没找到这个文件，就一当前epochOfZxid
            	LOG.info(CURRENT_EPOCH_FILENAME
            	        + " not found! Creating with a reasonable default of {}. This should only happen when you are upgrading your installation",
            	        currentEpoch);
            	writeLongToFile(CURRENT_EPOCH_FILENAME, currentEpoch);
            }
            if (epochOfZxid > currentEpoch) {
                //说明数据不是最新的
                throw new IOException("The current epoch, " + ZxidUtils.zxidToString(currentEpoch) + ", is older than the last zxid, " + lastProcessedZxid);
            }
            try {
                acceptedEpoch = readLongFromFile(ACCEPTED_EPOCH_FILENAME);
            } catch(FileNotFoundException e) {
            
            	writeLongToFile(ACCEPTED_EPOCH_FILENAME, acceptedEpoch);
            }
            if (acceptedEpoch < currentEpoch) {
                throw new IOException("The accepted epoch, " + ZxidUtils.zxidToString(acceptedEpoch) + " is less than the current epoch, " + ZxidUtils.zxidToString(currentEpoch));
            }
```



6:`org.apache.zookeeper.server.ZKDatabase#loadDataBase`

```java
 public long loadDataBase() throws IOException {
        long zxid = snapLog.restore(dataTree, sessionsWithTimeouts, commitProposalPlaybackListener);
        initialized = true;
        return zxid;
    }
    
  //主要还是要看下snapLog.restore方法，很明显，这是一个快照重新加载的方法，我记得zk的数据是会定时更新快照		//dataDir 这个文件会在加载zoo.cfg的时候初始化好。
  //那么说明快照中的数据就是新的，只要启动的时候，重新加载就可以。暂时这样理解吧。

 public long restore(DataTree dt, Map<Long, Integer> sessions,
                        PlayBackListener listener) throws IOException {
     	//看这个方法就好了
        long deserializeResult = snapLog.deserialize(dt, sessions);
        FileTxnLog txnLog = new FileTxnLog(dataDir);
        if (-1L == deserializeResult) {
 
            if (txnLog.getLastLoggedZxid() != -1) {
            
            save(dt, (ConcurrentHashMap<Long, Integer>)sessions);
            /* return a zxid of zero, since we the database is empty */
            return 0;
        }
        return fastForwardFromEdits(dt, sessions, listener);
           
  public long deserialize(DataTree dt, Map<Long, Integer> sessions)
            throws IOException {
        //这里已经按照版本进行排序了，最多100个哦
        List<File> snapList = findNValidSnapshots(100);
        if (snapList.size() == 0) {
            return -1L;
        }
        File snap = null;
        boolean foundValid = false;
        for (int i = 0, snapListSize = snapList.size(); i < snapListSize; i++) {
            snap = snapList.get(i);
            LOG.info("Reading snapshot " + snap);
            try (InputStream snapIS = new BufferedInputStream(new FileInputStream(snap));
                 CheckedInputStream crcIn = new CheckedInputStream(snapIS, new Adler32())) {
                InputArchive ia = BinaryInputArchive.getArchive(crcIn);
                //这个方法就不再进行展开看了，路面就是把节点zkNode加载出来呀，放到DataTree中去。
                deserialize(dt, sessions, ia);
                long checkSum = crcIn.getChecksum().getValue();
                long val = ia.readLong("val");
                if (val != checkSum) {
                    throw new IOException("CRC corruption in snapshot :  " + snap);
                }
                foundValid = true;
                break;
            } catch (IOException e) {
                LOG.warn("problem reading snap file " + snap, e);
            }
        }
        if (!foundValid) {
            throw new IOException("Not able to find valid snapshots in " + snapDir);
        }
        //最后这个snap是最新的
        dt.lastProcessedZxid = Util.getZxidFromName(snap.getName(), 		                          SNAPSHOT_FILE_PREFIX);
        return dt.lastProcessedZxid;
    }
```

上面这个过程就是zk服务重启后，是如何保证数据和先前一致性。

7：`org.apache.zookeeper.server.NIOServerCnxnFactory`在org.apache.zookeeper.server.quorum.QuorumPeer#startServerCnxnFactory中有调用这个方法。最后会调用到这里。

```java
 @Override
    public void start() {
        stopped = false;
        if (workerPool == null) {
            //工作线程池
            workerPool = new WorkerService(
                "NIOWorker", numWorkerThreads, false);
        }
        //工作线程启动
        for(SelectorThread thread : selectorThreads) {
            if (thread.getState() == Thread.State.NEW) {
                thread.start();
            }
        }
        // ensure thread is started once and only once
        //AcceptThread启动
        if (acceptThread.getState() == Thread.State.NEW) {
            acceptThread.start();
        }
        if (expirerThread.getState() == Thread.State.NEW) {
            expirerThread.start();
        }
```

* AcceptThread 

  ~~~java
  public void run() {
            try {
                while (!stopped && !acceptSocket.socket().isClosed()) {
                    try {
                          //这里看这个方法就好了
                         select(); //（1）
                     } catch (RuntimeException e) {
                          LOG.warn("Ignoring unexpected runtime exception", e);
                     } catch (Exception e) {
                          LOG.warn("Ignoring unexpected exception", e);
                     }
                  }
              } finally {
                 
              }
      
      private void select() {
              try {
                  selector.select();
                  Iterator<SelectionKey> selectedKeys =
                      selector.selectedKeys().iterator();
                  while (!stopped && selectedKeys.hasNext()) {//有连接
                      SelectionKey key = selectedKeys.next();
                      selectedKeys.remove();
  
                      if (!key.isValid()) {
                          continue;
                      }
                      if (key.isAcceptable()) {
                          if (!doAccept()) {//(2) 看这个方法就好了
                            
                              pauseAccept(10);
                          }
                      } else {
                          LOG.warn("Unexpected ops in accept select "
                                   + key.readyOps());
                      }
                  }
              } catch (IOException e) {
                  LOG.warn("Ignoring IOException while selecting", e);
              }
          }
      
       private boolean doAccept() {
              try {
                  sc = acceptSocket.accept();
                  accepted = true;
                  InetAddress ia = sc.socket().getInetAddress();//socket
                  int cnxncount = getClientCnxnCount(ia);
  
           
                  LOG.debug("Accepted socket connection from "
                           + sc.socket().getRemoteSocketAddress());
                  sc.configureBlocking(false);
  
                  // Round-robin assign this connection to a selector thread
                  if (!selectorIterator.hasNext()) {
                      selectorIterator = selectorThreads.iterator();
                  }
                  //还记得开始吗 我们说的这个selector线程组默认是两个，但这里怎么看，是不是发现也会
                  //只用到第一个线程 怎么感觉和Netty的bossGroup是一样的
                  SelectorThread selectorThread = selectorIterator.next();
                  //这里就不进去看了
                  if (!selectorThread.addAcceptedConnection(sc)) {
                      throw new IOException(
                          "Unable to add connection to selector queue"
                          + (stopped ? " (shutdown in progress)" : ""));
                  }
                  acceptErrorLogger.flush();
              } catch (IOException e) {
                  // accept, maxClientCnxns, configureBlocking
                  acceptErrorLogger.rateLimitLog(
                      "Error accepting new connection: " + e.getMessage());
                  fastCloseSock(sc);
              }
              return accepted;
          }
  ~~~

  

* `selectorThreads`工作线程组，里面是`SelectorThread`，这里看下这个线程

  ```java
  SelectorThread
       public SelectorThread(int id) throws IOException {
              super("NIOServerCxnFactory.SelectorThread-" + id);
              this.id = id;
      		//这个队列我觉得只有AcceptThread线程才会用到 只有处理连接的时候才会用到
              acceptedQueue = new LinkedBlockingQueue<SocketChannel>();
      		//
              updateQueue = new LinkedBlockingQueue<SelectionKey>();
          }
  		
  		//这个方法会把IO操作放到这个阻塞队列尾部，按理这里对于AcceptThread线程组来说，也是没用到的
  		 public boolean addInterestOpsUpdateRequest(SelectionKey sk) {
              if (stopped || !updateQueue.offer(sk)) {
                  return false;
              }
              wakeupSelector();
              return true;
          }
  
  	
  	
  
  ```

  **这里分析是针对zk自己与其他客户端的通信，就是默认端口2181，当然这个可以自己配置，但一般使用默认的，不知道上面理解对不对**

  `org.apache.zookeeper.server.NIOServerCnxnFactory.SelectorThread#handleIO`再看下这个方法

  ```java
   private void handleIO(SelectionKey key) {
              IOWorkRequest workRequest = new IOWorkRequest(this, key);
              NIOServerCnxn cnxn = (NIOServerCnxn) key.attachment();
  
              // Stop selecting this key while processing on its
              // connection
              cnxn.disableSelectable();
              key.interestOps(0);
              touchCnxn(cnxn);
              workerPool.schedule(workRequest);
          }
  	//看到这里应该理解很多了，就是会把请求封装成一个workRequest,然后放到workerPool线程池中去，这里就相当于一个业务线程。所以具体逻辑应该是在workRequest中doWork方法
          public void doWork() throws InterruptedException {
              if (!key.isValid()) {
                  selectorThread.cleanupSelectionKey(key);
                  return;
              }
  
              if (key.isReadable() || key.isWritable()) {
                  //接下来应该是要看这个方法哦
                  cnxn.doIO(key);
                  // Check if we shutdown or doIO() closed this connection
                  if (stopped) {
                      cnxn.close();
                      return;
                  }
                  if (!key.isValid()) {
                      selectorThread.cleanupSelectionKey(key);
                      return;
                  }
                  touchCnxn(cnxn);
              }
              // Mark this connection as once again ready for selection
              cnxn.enableSelectable();
              // Push an update request on the queue to resume selecting
              // on the current set of interest ops, which may have changed
              // as a result of the I/O operations we just performed.
              if (!selectorThread.addInterestOpsUpdateRequest(key)) {
                  cnxn.close();
              }
          }
  ```

  

  8：接下来要看的是NIOServerCnxn里面的方法`org.apache.zookeeper.server.NIOServerCnxn#doIO`

  从上面可以看到，doWork中会调用这个方法。

  ```java
  void doIO(SelectionKey k) throws InterruptedException {
          try {
           
              if (k.isReadable()) {//这里是读Io
                  int rc = sock.read(incomingBuffer);
                
                  if (incomingBuffer.remaining() == 0) {
                      if (isPayload) { // not the case for 4letterword
                          readPayload();//看这个方法（1）
                      }
                      else {
                   
                          return;
                      }
                  }
              }
              if (k.isWritable()) {//写IO
                  handleWrite(k);//(2)
              }
          } 
      }
      private void readPayload() throws IOException, InterruptedException {
          if (incomingBuffer.remaining() != 0) { // have we read length bytes?
              int rc = sock.read(incomingBuffer); // sock is non-blocking, so ok
              if (rc < 0) {
              }
          }
  
          if (incomingBuffer.remaining() == 0) { // have we read length bytes?
              packetReceived();
              incomingBuffer.flip();
              if (!initialized) {//没有初始化 那么就是连接请求
                  readConnectRequest();
              } else {
                  //真正的请求逻辑实现
                  readRequest();
              }
              lenBuffer.clear();
              incomingBuffer = lenBuffer;
          }
      }
  ```

  这里就分析到这里把，感觉自己有点绕进去了，总之上面写的都是节点启动的初始化，和节点直接的连接和通信。但还有问题没解决。比如ZookeeperServer是如何初始化的？但Server还没初始化呀，这个肯定是要等leader选举完成后才能确定的。

9：接下来要回到开始的时候了，还记得QuorumPeer的start方法吗，这里再把代码贴一下吧

```java
 @Override
    public synchronized void start() {
        if (!getView().containsKey(myid)) {
            throw new RuntimeException("My id " + myid + " not in the peer list");
         }
        loadDataBase();
        startServerCnxnFactory();
        try {
            adminServer.start();//这里暂时不管
        } catch (AdminServerException e) {
          
        }
        //到这里上面的其实已经分析的差不多了，接下来就是看这个方法
        startLeaderElection();
        super.start();
    }
```

zk中有LeaderElection/AuthFastLeaderElection/FastLeaderElection三种选举算法。默认的是FastLeaderElection。

`org.apache.zookeeper.server.quorum.QuorumPeer#startLeaderElection`

```java
 synchronized public void startLeaderElection() {
       try {
           //开始的状态是LOOKING
           if (getPeerState() == ServerState.LOOKING) {
               currentVote = new Vote(myid, getLastLoggedZxid(), getCurrentEpoch());
           }
       } catch(IOException e) {
          
           throw re;
       }
        if (electionType == 0) {
            try {
                udpSocket = new DatagramSocket(getQuorumAddress().getPort());
                responder = new ResponderThread();
                //这个地方我个人觉得就是一个心跳，把自己的节点状态广播出去
                responder.start();
            } catch (SocketException e) {
                throw new RuntimeException(e);
            }
        }
        this.electionAlg = createElectionAlgorithm(electionType);
    }

```



**问题：节点之间心跳是如何实现的**?

我觉得会是在sendWork和ReceiveWork中，但是没找到



10:`org.apache.zookeeper.server.quorum.FastLeaderElection`默认选举就在这里面实现的

```java
 private void starter(QuorumPeer self, QuorumCnxManager manager) {
        this.self = self;
        proposedLeader = -1;
        proposedZxid = -1;
		//看名字就知道这两个阻塞队列的意思，一个是用来发送信息的，一个是接收通知的
        sendqueue = new LinkedBlockingQueue<ToSend>();
        recvqueue = new LinkedBlockingQueue<Notification>();
     	//这个messenger是比较关键的
        this.messenger = new Messenger(manager);
    }
	
	//Messenger 构造方法
	 Messenger(QuorumCnxManager manager) {
			//sender线程，用来发送sendqueue这个队列里面的任务
            this.ws = new WorkerSender(manager);
		
            this.wsThread = new Thread(this.ws,
                    "WorkerSender[myid=" + self.getId() + "]");
            this.wsThread.setDaemon(true);
			
         	//接收线程
            this.wr = new WorkerReceiver(manager);

            this.wrThread = new Thread(this.wr,
                    "WorkerReceiver[myid=" + self.getId() + "]");
            this.wrThread.setDaemon(true);
        }
		//最后启动上面这两个线程
 		void start(){
            this.wsThread.start();
            this.wrThread.start();
        }

```

 * WorkerSender/WorkerReceiver   SendWorker/RecvWorker 把这两组线程搞懂，就能明白节点直接消息的通信

   服务节点会把要notify的信息，封装撑一个ToSend对象,放到workerSender的队列中，然后在process()方法中构建要放得信息ByteBuffer,，接着是`org.apache.zookeeper.server.quorum.QuorumCnxManager#toSend`调用这个方法，把消息加到sid对应的ConcurrentHashMap<Long, ArrayBlockingQueue<ByteBuffer>> queueSendMap中。如果是第一次，那么就需要建立socket连接，具体可以看下connectOne(sid)这个方法。

   ```java
   //里面很多代码已经被我删除了，就留下核心代码
   synchronized private boolean connectOne(long sid, InetSocketAddress electionAddr){
           if (senderWorkerMap.get(sid) != null) {
               //说明这个连接已经存在了
               LOG.debug("There is a connection already for server " + sid);
               return true;
           }
           Socket sock = null;
           try {
               	//简历socket连接
                    sock = new Socket();
                    setSockOpts(sock);
                    sock.connect(electionAddr, cnxTO);
               
               if (quorumSaslAuthEnabled) {
                   //异步连接
                   //我猜这里面肯定是要把这份socket连接缓存，并且肯定是有一天线程来专门处理
                   initiateConnectionAsync(sock, sid);
               } else {
                   //同步连接
                   initiateConnection(sock, sid);
               }
               return true;
       }
       
       //就分析上面异步简历连接的这个方法，和同步最终结果应该是一样的，异步最后也会调用同步的这方法，就是简单的专门放到了另一个线程池去处理
        public void initiateConnectionAsync(final Socket sock, final Long sid) {
           if(!inprogressConnections.add(sid)){
               closeSocket(sock);
               return;
           }
           try {
               //很明显，这是专门一个线程池来处理的。所以接下来看QuorumConnectionReqThread
               connectionExecutor.execute(
                       new QuorumConnectionReqThread(sock, sid));
               connectionThreadCnt.incrementAndGet();
           }
       }
   
       //下面这个方法是最终建立连接后，可以完成通信了
        private boolean startConnection(Socket sock, Long sid)
               throws IOException {
           DataOutputStream dout = null;
           DataInputStream din = null;
           try {
           
               dout = new DataOutputStream(buf);
   
               dout.writeLong(PROTOCOL_VERSION);
               dout.writeLong(self.getId());
               final InetSocketAddress electionAddr = self.getElectionAddress();
               String addr = electionAddr.getHostString() + ":" + electionAddr.getPort();
               byte[] addr_bytes = addr.getBytes();
               dout.writeInt(addr_bytes.length);
               dout.write(addr_bytes);
               dout.flush();
   
               din = new DataInputStream(
                       new BufferedInputStream(sock.getInputStream()));
           } catch (IOException e) {
               return false;
           }
   
           QuorumPeer.QuorumServer qps = self.getVotingView().get(sid);
           if (qps != null) 
               authLearner.authenticate(sock, qps.hostname);
           }
   
           // If lost the challenge, then drop the new connection
           if (sid > self.getId()) {//TODO 这里暂时不懂什么意思
               LOG.info("Have smaller server identifier, so dropping the " +
                       "connection: (" + sid + ", " + self.getId() + ")");
               closeSocket(sock);
               // Otherwise proceed with the connection
           } else {
              	//这里看到sendWorker了吧，持有和一个节点的通信socket，最终的消息有这里发送
               SendWorker sw = new SendWorker(sock, sid);
               //同样RecvWorker,看到这里应该能想到，这个线程会把读任务放到
               //org.apache.zookeeper.server.quorum.QuorumCnxManager#recvQueue中
               //然后由WorkerReceiver线程来进行消耗
               RecvWorker rw = new RecvWorker(sock, din, sid, sw);
               sw.setRecv(rw);
   			
               SendWorker vsw = senderWorkerMap.get(sid);
   
               if(vsw != null)
                   vsw.finish();
   			//缓存到org.apache.zookeeper.server.quorum.QuorumCnxManager#senderWorkerMap
               senderWorkerMap.put(sid, sw);
               
               queueSendMap.putIfAbsent(sid, new ArrayBlockingQueue<ByteBuffer>(
                       SEND_CAPACITY));
               sw.start();
               rw.start();
   
               return true;
   
           }
           return false;
       }
       
   ```

   **注意这里的通信，应该是zk集群节点之间的通信**

   


### 客户端请求

因为客户端与zk建立连接是通过`AcceptThread`线程，并注册事件，之后的IO操作是`selectorThreads`线程处理，然后业务会放到`workPool`线程池中去做，线程中的线程是一个IOWorkRequest。接着会对数据进行读取，并解析成一个Request对象，再交给`RequestProcessor`,这是一个zk线程。run方法中会调用nextProcessor.processRequest(si)，很长，但逻辑还是比较清晰。其实我们大部分操作都是基于节点操作的，操作完之后，会发结果封装成一个Response,返回给客户端，应该每个Response都会继承`org.apache.jute.Record`,最后是通过` org.apache.zookeeper.server.ServerCnxn#sendResponse(r) ` 把结果返回给客户端。



### 节点之间数据同步问题



### zk节点创建是如何保证唯一性的







